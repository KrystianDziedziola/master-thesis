\chapter{Rozwiπzanie problemu}
\label{chapter_5}
Proces uczenia sieci neuronowej stworzony zosta≥ w úrodowisku Jupyter Notebook. Umoøliwia ono tworzenie dokumentÛw zawierajπcych opisy, wykresy oraz wykonywalne kody ürÛd≥owe. Nazwa Jupyter wziÍ≥a siÍ z tego, øe úrodowisko umoøliwia pracÍ w trzech jÍzykach: Julia, Python oraz R.

Dokument sk≥ada siÍ z wielu komÛrek, ktÛre mogπ byÊ wywo≥ywane niezaleønie.
Przewagπ kodu pisanego w tym úrodowisku nad tradycyjnymi skryptami jest oferowana przez nie moøliwoúÊ zapamiÍtywania stanu zmiennych. DziÍki temu nie ma potrzeby kaødorazowego wywo≥ywania ca≥ego kodu, a jedynie fragmentÛw, ktÛre w danej chwili sπ potrzebne np. jednorazowe przygotowanie danych, a nastÍpnie wykonywanie jedynie procesu uczenia.

Proces uczenia sk≥ada siÍ z kilku krokÛw:
\begin{itemize}
	\item pobranie i przygotowanie danych za pomocπ przygotowanych skryptÛw \ref{data_preparation},
	\item podzia≥ danych na zbiory s≥uøπce do nauki oraz testowania parzy pomocy k-krotnej 
walidacji krzyøowej \textit{(ang. k-fold cross-validation)} \ref{validation_method},
	\item utworzenie modelu sieci neuronowej \ref{build_model},
	\item rozpoczÍcie uczenia modelu \ref{run_learning},
	\item sprawdzenie skutecznoúci i przedstawienie wynikÛw \ref{verify_accuracy}.
\end{itemize}

Wszystkie wymienione kroki przedstawiajπ pe≥en proces uczenia, ktÛry pokazuje z jakπ dok≥adnoúciπ model jest w stanie wykrywaÊ stany padaczkowe.

\section{Przygotowanie danych}
\label{data_preparation}
W celu wykorzystania dostÍpnych danych w procesie uczenia sieci neuronowej naleøy je najpierw odpowiednio przygotowaÊ.
Dostarczone zosta≥y dane w postaci plikÛw tekstowych z liczbami reprezentujπcymi wartoúci zmierzone przy pomocy elektroencefalogramu. W poszczegÛlnych kolumnach znajdujπ siÍ wartoúci rÛønic potencja≥Ûw pomiÍdzy dwoma elektrodami dla 16 kana≥Ûw. 
Jest to bardzo typowy uk≥ad wynikajπcy z rozmieszczenia elektrod na g≥owie pacjenta - tzw. system 10-20 (patrz rysunek \ref{fig:10_20}).

Nag≥Ûwki kolumn poszczegÛlnych kana≥Ûw posiadajπ etykiety \textit{EEG\_FP1\_F3}, \textit{EEG\_FP2\_F4} itd. 
Przyk≥adowo, \textit{EEG\_FP1\_F3} oznacza sygna≥ rÛønicowy pomiÍdzy elektrodami \textit{FP1} oraz \textit{F3}.  Ostatnia kolumna (o nag≥Ûwku \textit{t}) przedstawia czas w sekundach, w ktÛrym mia≥ miejsce pomiar. 

\begin{figure}[h!]
	\centering
	\includegraphics[width=10cm]{Rysunki/Rozdzial2/10-20.png}
	\caption{Rozmieszczenie elektrod w systemie 10-20. èrÛd≥o: \cite{10_20}}
	\label{fig:10_20}
\end{figure}

DostÍpne sπ dane 104 pacjentÛw, u ktÛrych wystπpi≥y ataki. Kaødy z nich posiada odczyty wykonane z czÍstotliwoúciπ 500Hz przez blisko 15 minut. Jest to oko≥o 450 000 linii w kaødym pliku. Dane zajmujπ 6,5 GB.
Fragment jednego z plikÛw przedstawiony zosta≥ na listingu \ref{lst:eeg}:
\begin{landscape}
\begin{lstlisting}[caption=Dane liczbowe z odczytu EEG, label=lst:eeg]
"EEG_FP1_F3" "EEG_FP2_F4" "EEG_F3_C3" "EEG_F4_C4" "EEG_C3_P3" "EEG_C4_P4" "EEG_P3_O1" "EEG_P4_O2" "EEG_FP1_F7" "EEG_FP2_F8" "EEG_F7_T3" "EEG_F8_T4" "EEG_T3_T5" "EEG_T4_T6" "EEG_T5_O1" "EEG_T6_O2" "t"
-1.7032  -3.3727 5.9014 2.512 2.6404 4.0193 16.9162 35.4401 5.3506 1.1355 -1.0843 -3.9932 9.4594 14.8409 10.0322 26.6143 0
-3.1702 -4.5883 5.8762 2.9638 3.5952 3.9019 17.1137 36.3634 4.3875 0.543 -1.356 -4.5697 10.309 14.1082 10.0724 28.5593 0.002
-4.2669 -5.2157 5.6852 3.1698 4.0053 3.6091 16.4135 36.7024 2.9881 0.4522 -1.7584 -5.4206 11.1134 13.3778 9.4926 29.8509 0.004
-4.8106 -5.2287 5.2889 2.6745 3.714 3.0257 14.9097 36.4086 1.6189 0.2802 -2.1206 -6.4522 11.2876 12.7727 8.3198 30.2831 0.006
-5.0684 -5.1176 4.7665 1.7354 2.9116 2.354 12.7327 35.6144 0.5718 -0.618 -2.57 -7.3378 10.6144 12.2487 6.7256 30.2934 0.008
-5.3637 -5.1568 4.399 1.03 1.764 1.9509 10.2011 34.6652 -0.2974 -2.1804 -3.1603 -7.7025 9.3368 11.727 5.1179 30.6433 0.01
-5.6214 -5.0654 4.3918 0.9508 0.24 1.8783 7.7188 33.8484 -1.2102 -3.7284 -3.7707 -7.4767 7.7603 11.2122 3.9518 31.6004 0.012
-5.3918 -4.3726 4.6296 1.2678 -1.7391 1.9194 5.4296 32.8928 -2.0525 -4.7365 -4.371 -6.9662 6.0031 10.7833 3.3565 32.6295 0.014
-4.4216 -2.9806 4.8926 1.4897 -4.0924 1.8759 3.1044 31.2302 -2.5055 -5.2621 -5.1323 -6.5737 4.1319 10.4842 2.993 32.964 0.016
-3.0671 -1.3206 5.1772 1.4303 -6.5959 1.8178 0.4606 28.7023 -2.4988 -5.6443 -6.219 -6.4834 2.3209 10.2523 2.3732 32.5009 0.018
-2.0125 0.0126 5.624 1.3708 -9.0008 1.8601 -2.4571 25.9613 -2.4216 -6.0744 -7.4599 -6.5563 0.7207 10.0158 1.3118 31.8217 0.02
-1.6282 0.6792 6.1788 1.668 -11.1278 1.9872 -5.285 23.879 -2.7505 -6.5044 -8.4291 -6.466 -0.7268 9.7492 0.0498 31.4306 0.022
-1.6657 0.8034 6.6003 2.2822 -12.8962 2.0405 -7.673 22.7264 -3.4955 -6.877 -8.9121 -5.952 -2.1528 9.4663 -1.074 31.2145 0.024
-1.5719 0.7576 6.7409 2.7736 -14.344 1.9775 -9.5717 21.9936 -4.0627 -7.2927 -9.1401 -5.0143 -3.5228 9.1023 -2.0194 30.7051 0.026
-1.0657 0.7511 6.7409 2.833 -15.5319 1.8759 -11.2415 20.9346 -3.915 -7.9234 -9.4655 -3.9029 -4.6154 8.5574 -3.1008 29.6605 0.028
-0.4658 0.64 6.7481 2.6388 -16.4105 1.8674 -12.9338 19.1106 -3.3982 -8.8312 -9.7774 -2.8922 -5.2413 7.7923 -4.6393 28.1837 0.03
-0.4283 0.1433 6.6724 2.5675 -16.8789 1.9739 -14.6844 16.5569 -3.66 -9.9301 -9.5996 -2.0969 -5.4112 6.8556 -6.6483 26.4136 0.032
-1.2579 -0.837 6.2977 2.6904 -17.0425 2.1591 -16.2868 13.5512 -5.3513 -11.1628 -8.8048 -1.3918 -5.2607 5.7612 -8.8668 24.3656 0.034
-2.5281 -2.1571 5.5879 2.726 -17.2241 2.3092 -17.4808 10.3421 -7.6669 -12.6201 -7.9998 -0.5653 -4.9359 4.4372 -11.0386 21.9677 0.036
-3.4233 -3.5492 4.6692 2.5041 -17.6387 2.3055 -18.1227 6.9393 -9.0697 -14.3688 -7.8724 0.5427 -4.5573 2.8026 -13.0119 19.2251 0.038
-3.5686 -4.719 3.6748 2.1871 -18.0668 2.0102 -18.226 3.2427 -9.0093 -16.2083 -8.3252 1.8902 -4.2347 0.9176 -14.6128 16.1274 0.04
-3.292 -5.4248 2.7057 1.9731 -18.0892 1.3869 -17.9791 -0.8025 -8.4321 -17.699 -8.5968 3.1857 -3.9831 -0.9698 -15.6362 12.613 0.042
-3.1795 -5.5751 2.0283 1.7869 -17.5715 0.4949 -17.5975 -5.0931 -8.516 -18.5016 -8.1441 4.0158 -3.6433 -2.5673 -16.0108 8.6663 0.044
\end{lstlisting}
\end{landscape}

Dodatkowo dostarczony zosta≥ plik przechowujπcy wyznaczone przez lekarza momenty wystπpienia atakÛw dla kaødego z pacjentÛw. Plik w kolejnych wierszach zawiera punkty w czasie rozdzielone przecinkami (w sekundach).
Kaødy wiersz odpowiada jednemu pacjentowi, dlatego plik zawiera 104 wiersze. Fragment pliku z czasami wystπpienia atakÛw:
\begin{lstlisting}[caption=Punkty w czasie wystπpienia atakÛw (w sekundach)] 
835,853,865,873,889,908
18,48,110,309,466,618,757
216,239
44,329,501,559,622
36,190,406,576,714,754
158,510,917
622,653,676,737
\end{lstlisting}

Dane z kilku kana≥Ûw z naniesionym momentem wystπpienia ataku przedstawione zosta≥y na rysunku \ref{fig:data_vis}.
Moøna zaobserowaÊ, øe bez posiadania specjalistycznej wiedzy ciÍøko by≥oby okreúliÊ moment wystπpienia ataku. 
Fragmenty odczytu w momencie ataku na pierwszy rzut oka nieznacznie rÛøniπ siÍ od pozosta≥ych.
åwiadczy o wiele wiÍkszej z≥oøonoúci problemu w porÛwnaniu do zadaÒ, z ktÛrymi mÛzg praktycznie kaødego cz≥owieka radzi sobie doskonale np. rozpoznawanie liczb, odrÛønianie psÛw od kotÛw itp. Wykresy przedstawiajπce odczyty ze wszystkich 16 kana≥Ûw zaprezentowano na rysunku \ref{fig:16_channels}.

\begin{landscape}
\begin{figure}[h!]
	\centering
	\includegraphics[width=25cm]{Rysunki/Rozdzial2/eeg_vis.png}
	\caption{Wykres przedstawiajπcy odczyt EEG z zaznaczonym momentem wystπpienia ataku}
	\label{fig:data_vis}
\end{figure}
\end{landscape}

\begin{figure}[h!]
	\centering
	\includegraphics[width=15cm]{Rysunki/Rozdzial5/16channels.png}
	\caption{Odczyty ze wszystkich 16 kana≥Ûw z zaznaczonymi miejscami wystπpienia atakÛw}
	\label{fig:16_channels}
\end{figure}

W celu wykorzystania przedstawionych danych w procesie uczenia sieci neuronowej muszπ zostaÊ one odpowiednio przygotowane.
Wybrane podejúcie zak≥ada podzielenie danych na okna czasowe o okreúlonej d≥ugoúci. Znane sπ jedynie poczπtki atakÛw, jednak nie wiadomo jak d≥ugo trwa≥y. D≥ugoúÊ okna czasowego, wed≥ug ktÛrego podzielone zostanπ dane bÍdzie wiÍc jednym z parametrÛw, ktÛry naleøy dobraÊ w celu uzyskania najlepszych rezultatÛw.

Do przygotowania danych stworzony zosta≥ skrypt, ktÛry wykonuje nastÍpujπce czynnoúci:
\begin{itemize}
	\item wczytanie danych pacjentÛw oraz informacji o atakach,
	\item przetworzenie danych do czÍstotliwoúci 100Hz,
	\item podzia≥ danych na okna czasowe o d≥ugoúci przekazanej jako parametr (w sekundach),
	\item normalizacja danych - úrednia 0, odchylenie standardowe 1,
	\item konwersja danych dotyczπcych czasu wystπpienia atakÛw na postaÊ tzw. "jeden z n" \textit{(ang. one-hot encoding)},
	\item wyúwietlanie informacji o postÍpie przetwarzanych plikÛw,
	\item zapis pobranych danych do plikÛw tymczasowych umoøliwiajπcych szybszy odczyt.
\end{itemize}

Technicznie skrypty zosta≥y podzielone na 2 pliki:
\begin{itemize}
\item data\_reader.py,
\item chunks\_creator.py.
\end{itemize}

Pierwszy z nich zajmuje siÍ wczytywaniem i korzysta z drugiego w celu stworzenia okien czasowych.
G≥Ûwnπ funkcjπ dostarczanπ przez skrypt, ktÛra umoøliwia wykonanie wszystkich wymienionych wyøej czynnoúci jest \textit{get\_data()}. Jako parametr przyjmuje ona czas w sekundach, ktÛry oznacza d≥ugoúÊ okna czasowego. Znajduje siÍ ona na koÒcu pliku, gdyø z uwagi na specyfikÍ jÍzyka wszystkie wykorzystywane przez niπ funkcje muszπ byÊ wczeúniej zadeklarowane. 
Kod skryptu pobierajπcego dane rozszerzony o komentarze opisujπce poszczegÛlne kroki zaprezentowany zosta≥ na listingu \ref{lst:data-reader.py}.

\begin{lstlisting}[caption=data\_reader.py, language=Python, label={lst:data-reader.py}]
import os
import numpy as np
import pickle

from chunks_creator import prepare_chunks
from chunks_creator import flatten_chunks

from sklearn.preprocessing import StandardScaler

INPUT_DATA_FILE_PATH='tmp/input-{}sec.pckl'

DATA_FREQUENCY = 500
SAMPLING_RATE = 5
FREQUENCY_TO_SAMPLING_RATIO = DATA_FREQUENCY // SAMPLING_RATE


# Konwertuje plik z danymi pacjenta z postaci tekstowej do tablicowej oraz zmienia czÍstotliwoúÊ prÛbkowania do 100Hz
def parse_file(file, sampling_rate):
    lines = file.split('\n')
    headers = lines[0].split('\t')
    # to one before last because the last one is empty
    data = lines[1:-1]

    number_of_lines = len(data)

    float_data = np.zeros((number_of_lines, len(headers)))
    for line_number, line in enumerate(data):
        values = [float(value) for value in line.split('\t')]
        float_data[line_number, :] = values

    return float_data[::sampling_rate], headers


# Wczytuje dane pacjentÛw z dysku
def read_input_files(end, data_path, sampling_rate):
    input_path = os.path.join(data_path, 'input_500Hz/sick')
    input_file_names = os.listdir(input_path)
    input_file_names.sort(key=int)

    start = None

    files_content = []
    for file_name in input_file_names[start:end]:
        file_path = os.path.join(input_path, file_name)
        file = open(file_path, 'r')
        (columns, headers) = parse_file(file.read(), sampling_rate)
        print('Loaded input file:', file_name)
        file.close()
        files_content.append(columns)
    print('--Input files loaded--')
    return files_content, headers


def create_target_index(value, frequency_to_sampling_ratio):
    value = int(value)
    return int(value * frequency_to_sampling_ratio)


# Odczytuje informacje dotyczπce czasÛw atakÛw i konwertuje do postaci one-hot
def read_target_files(end, data_path, sampling_rate, data_frequency):
    frequency_to_sampling_ratio = data_frequency // sampling_rate
    targets_path = os.path.join(data_path, 'targets')
    targets_file_name = os.listdir(targets_path)[0]
    targets_file_path = os.path.join(targets_path, targets_file_name)

    file = open(targets_file_path, 'r')
    targets_content = file.read()
    file.close()

    lines = targets_content.split('\n')[:-1]
    targets = []
    for number, line in enumerate(lines, 1):
        targets.append([(int(value), create_target_index(value, frequency_to_sampling_ratio)) for value in line.split(',')])
    print('--Target files loaded--')
    return targets[:end]


# Pobiera dane pacjentÛw oraz czasy atakÛw
def read_data(data_path, sampling_rate, data_frequency, end=104):
    (input_data, headers) = read_input_files(end, data_path, sampling_rate)
    targets_data = read_target_files(end, data_path, sampling_rate, data_frequency)

    return input_data, targets_data, headers


# Odczytuje dane i zapisuje zmienne do pliku tymczasowego
def load_data_to_file(chunk_size_in_seconds):
    (input_data, target, headers) = read_data(data_path='data', 
                                              sampling_rate=SAMPLING_RATE, 
                                              data_frequency=DATA_FREQUENCY)

    with open(INPUT_DATA_FILE_PATH.format(chunk_size_in_seconds), 'wb') as input_variable_file:
        pickle.dump([input_data, target, headers], input_variable_file)

    del input_data, target, headers
    

# Normalizuje dane uøywajπc obiektu StandardScaler(). ZwrÛcone dane posiadajπ úredniπ 0 oraz odchylenie standardowe 1.    
def normalize(x, y):
    scalers = {}
    for channel_number in range(x.shape[1]):
        scalers[channel_number] = StandardScaler()
        x[:, channel_number, :] = scalers[channel_number].fit_transform(x[:, channel_number, :]) 
    return x, y.astype(int)


# Pobiera zmienne z danymi z utworzonego pliku tymczasowego
def load_input_data(chunk_size_in_seconds):
    with open(INPUT_DATA_FILE_PATH.format(chunk_size_in_seconds), 'rb') as input_data_file:
        input_data, target, headers = pickle.load(input_data_file)
    
    return input_data, target, headers


# Pobiera dane wykorzystujπc funkcjÍ load_input_data() i wywo≥uje funkcjÍ prepare_chunks() z pliku chunks_creator.py w celu utworzenia okien czasowych z danymi. Utworzone porcje danych sπ nastÍpnie poddawane normalizacji.
def prepare_data(chunk_size_in_seconds):
    input_data, target, headers = load_input_data(chunk_size_in_seconds)
    
    chunks_input, chunks_target = prepare_chunks(input_data, 
                                                target, 
                                                chunk_size_in_seconds=chunk_size_in_seconds, 
                                                ratio=FREQUENCY_TO_SAMPLING_RATIO)
    x, y = flatten_chunks(chunks_input, chunks_target)
    x, y = normalize(x, y)
    
    return x, y


# G≥Ûwna funkcja skryptu, ktÛra zwraca odpowiednio przygotowane dane. Przy pierwszym uruchomieniu wczytuje dane z dysku do zmiennych i zapisuje je do pliku tymczasowego. Odczyt zmiennych z danymi z pliku binarnego umoøliwia szybszy dostÍp przy kolejnych uruchomieniach, gdyø otwieranie duøych plikÛw tekstowych jest czasoch≥onne.
def get_data(chunk_size_in_seconds):
    file_exists = os.path.isfile(INPUT_DATA_FILE_PATH.format(chunk_size_in_seconds))
    
    if not file_exists:
        load_data_to_file(chunk_size_in_seconds)
        
    return prepare_data(chunk_size_in_seconds)
\end{lstlisting}

Funkcja \textit{prepare\_data()} w powyøszym skrypcie korzysta z osobnego pliku o nazwie \textit{chunks\_creator.py}, w ktÛrym zosta≥y zdefiniowane funkcje tworzπce okna czasowe z danymi.
Dla danych kaødego z pacjentÛw tworzonych jest \textit{2 * n} okien czasowych, gdzie \textit{n} oznacza iloúÊ zdiagnozowanych atakÛw. 

Porcje danych zawieracjπce ataki tworzone sπ na poczπtku kaødego z nich i trwajπ przez okreúlony czas podany w parametrze. NastÍpnie tworzone jest \textit{n} kolejnych porcji danych zawierajπcych dane liczbowe z okresu, w ktÛrym nie wystπpi≥ atak.
Utworzone okna czasowe zawierajπ wiÍc takπ samπ iloúÊ danych z atakami oraz bez.
Dodatkowo tworzona jest tablica zawierajπca informacje o tym czy dla danego okna czasowego wystπpi≥  \textit{(wartoúÊ 1)} lub nie wystπpi≥ \textit{(wartoúÊ 0)} atak.

Implementacja skryptu odopowiadajπcego za przetwarzanie danych do postaci okien czasowych przedstawiona zosta≥a na listlingu \ref{lst:chunks-creator.py}.

\begin{lstlisting}[caption=chunks\_creator.py, language=Python, label={lst:chunks-creator.py}]
import random
import numpy as np


# Tworzy porcje danych, w ktÛrych wystπpi≥y ataki.
def create_chunks_with_seizures(patient_data, seizure_seconds, chunk_size):
    number_of_chunks = len(seizure_seconds)

    chunks_input = np.zeros((number_of_chunks, chunk_size, 17))
    chunks_target = np.zeros(number_of_chunks)

    for seizure_number in range(0, number_of_chunks):
        (seizure_time, seizure_index) = seizure_seconds[seizure_number]
        chunk_start_index = seizure_index
        chunk_end_index = chunk_start_index + chunk_size
        chunks_input[seizure_number] = patient_data[chunk_start_index:chunk_end_index, :]
        # atak oznaczony wartoúciπ '1'
        chunks_target[seizure_number] = 1

    return (chunks_input, chunks_target)


# Sprawdza czy podany fragment znajduje siÍ w zasiÍgu ataku.
def is_in_seizure_range(index, seizure_seconds, chunk_size):
    for (seizure_time, seizure_index) in seizure_seconds:
        seizure_start_index = seizure_index
        seizure_end_index = seizure_start_index + chunk_size
        if index in range(seizure_start_index, seizure_end_index):
            return True

    return False


# Tworzy poczπtek pojedynczej porcji danych, ktÛry wybierany jest losowo, jednak sprawdzane jest, aby nie zawiera≥a ona momentÛw, w ktÛrych wystπpi≥ atak.
def create_non_seizure_data_start_index(data_size, chunk_size, seizure_seconds):
    start_index = random.randint(0, data_size - chunk_size)

    while (is_in_seizure_range(start_index, seizure_seconds, chunk_size)):
        start_index = random.randint(0, data_size - chunk_size)

    return start_index


# Tworzy porcje danych, w ktÛrych nie wystπpi≥y ataki.
def create_chunks_without_seizures(patient_data, seizure_seconds, chunk_size):
    number_of_chunks = len(seizure_seconds)

    chunks_input = np.zeros((number_of_chunks, chunk_size, 17))
    chunks_target = np.zeros(number_of_chunks)
    (data_size, channels) = patient_data.shape

    for chunk_number in range(0, number_of_chunks):
        chunk_start_index = create_non_seizure_data_start_index(data_size, chunk_size, seizure_seconds)

        chunk_end_index = chunk_start_index + chunk_size
        chunks_input[chunk_number] = patient_data[chunk_start_index:chunk_end_index, :]
        # brak ataku oznaczone wartoúciπ '0'
        chunks_target[chunk_number] = 0

    return (chunks_input, chunks_target)


# Przystosowuje iloúÊ wymiarÛw danych do procesu uczenia sieci neuronowej.
def flatten_chunks(chunks_input, chunks_target):
    train_input = []
    train_target = []

    for patient_number in range(0, len(chunks_input)):
        patient_data = chunks_input[patient_number]
        patient_targets = chunks_target[patient_number]
        for chunk_number in range(0, len(patient_data)):
            train_input.append(patient_data[chunk_number])
            train_target.append(patient_targets[chunk_number])

    train_input = np.array(train_input)
    train_target = np.array(train_target)

    train_input = train_input[:, :, :-1]
    
    return train_input, train_target 


# G≥Ûwna funkcja skryptu zwracajπca okna czasowe stworzone z danych pacjentÛw w postaci wielowymiarowej tablicy oraz tablicÍ zawierajπcπ informacje o wystπpieniu atakÛw.
def prepare_chunks(input, target, chunk_size_in_seconds, ratio):
    chunk_size = chunk_size_in_seconds * ratio
    chunks_input = []
    chunks_target = []

    for patient_number in range(0, len(input)):
        patient_chunks_input = []
        patient_chunks_target = []
        seizure_seconds = target[patient_number]
        patient_data = input[patient_number]
        (seizure_chunks_input, seizure_chunks_target) = create_chunks_with_seizures(patient_data,seizure_seconds, chunk_size)
        patient_chunks_input.extend(seizure_chunks_input)
        patient_chunks_target.extend(seizure_chunks_target)

        (non_seizure_chunks_input, non_seizure_chunks_target) = create_chunks_without_seizures(patient_data, seizure_seconds, chunk_size)
        patient_chunks_input.extend(non_seizure_chunks_input)
        patient_chunks_target.extend(non_seizure_chunks_target)

        chunks_input.append(np.array(patient_chunks_input))
        chunks_target.append(np.array(patient_chunks_target))

    return np.array(chunks_input), np.array(chunks_target)

\end{lstlisting}

Przedstawione skrypty umoøliwiajπ pobranie odpowiednio przygotowanych danych, ktÛre nastÍpnie mogπ byÊ uøyte w procesie uczenia sieci neuronowej.

\section{Metoda oceny wynikÛw}
\label{validation_method}
Do zweryfikowania skutecznoúci modelu wymagane jest podzielenie danych na odpowiednie zbiory. Korzystanie z jednego, identycznego zbioru danych do nauki oraz testowania jest b≥Ídem, gdyø moøe wprowadziÊ z≥udne wraøenie, øe model posiada bardzo dobrπ skutecznoúÊ. SieÊ neuronowa, ktÛrej dzia≥anie weryfikowane jest na danych, ktÛre pos≥uøy≥y do nauki bÍdzie posiadaÊ skutecznoúÊ bliskπ 100\%. Dzieje siÍ tak dlatego, øe umie ona rozpoznawaÊ te konkretne dane, gdyø zna dla nich wartoúci wyjúciowe, ktÛre podane zosta≥y podczas nauki. W ten sposÛb nauczony model bÍdzie natomiast posiada≥ bardzo niskπ skutecznoúÊ, gdy na jego wejúcie podane zostanπ dane, ktÛre nigdy wczeúniej nie zosta≥y mu przedstawione.
W kaødym rodzaju uczenia maszynowego dπøy siÍ do uzyskania jak najwiÍkszej zdolnoúci generalizacji, czyli skutecznoúci klasyfikacji danych, ktÛre nie zosta≥y nigdy wczeúniej przekazane do modelu. PrzeciwieÒstwem tego jest tzw. \textit{nadmierne dopasowanie} (ang. \textit{overfitting}), czyli sytuacja, w ktÛrej model zbytnio dopasowuje siÍ do danych uczπcych, co powoduje bardzo s≥abπ zdolnoúÊ generalizacji. Zjawisko to jest g≥Ûwnym problemem podczas uczenia modelu.

\subsection{Hold-out}
Podstawowym sposobem oceny wynikÛw jest podzielenie danych na dwa zbiory: treningowy oraz testowy \textit{(ang. hold-out)}. Moøna dzieliÊ je w rÛønych proporcjach np. 80/20, 90/10, 95/5 itp. w zaleønoúci od iloúci posiadanych danych. Przy niewielkich zbiorach wydzielenie zbyt duøej liczby danych do zbioru testowego moøe powodowaÊ niedobory w procesie nauki. Wydzielenie jedynie dwÛch zbiorÛw danych nie rozwiπzuje jednak do koÒca problemu \textit{overfitting}'u, gdyø zbyt czÍsta zmiana modelu i sprawdzanie jego skutecznoúci na danych testowych moøe spowodowaÊ zbyt duøe dopasowanie do zbioru testowego \cite{Chollet:2018}. Z tego wzglÍdu czÍsto stosowany jest dodatkowy zbiÛr walidacyjny \textit{(ang. validation set)}. W tym podejúciu model uczony jest na danych treningowych, sprawdzany na danych walidacyjnych i na podstawie tych wynikÛw wprowadza siÍ modyfikacje w modelu. ZbiÛr testowy s≥uøy jedynie do ostatecznego sprawdzenia skutecznoúci modelu, gdyø sπ to dane, ktÛre nigdy wczeúniej nie zosta≥y uøyte w procesie nauki. Dane dzielone sπ w proporcjach np. 80/10/10, 90/5/5. Schemat tego podejúcia przedstawiony jest na rysunku \ref{fig:hold-out-validation}

\begin{figure}[h!]
	\centering
	\includegraphics[width=9cm]{Rysunki/Rozdzial5/hold-out-validation.pdf}
	\caption{Podzia≥ danych na zbiory: treningowy, walidacyjny i testowy}
	\label{fig:hold-out-validation}
\end{figure}

Metoda ta rozwiπzuje problem zbytniego dopasowywania siÍ do zbioru testowego, jednak rÛwnieø posiada wadÍ poprzedniego rozwiπzania. Przy niewielkiej iloúci danych wydzielanie osobnych zbiorÛw moøe spowodowaÊ, øe zbiÛr uczπcy bÍdzie zbyt ma≥y. W celu unikniÍcia tego problemu moøna zastosowaÊ tzw. walidacjÍ krzyøowπ i jej iteracyjne rozszerzenie.


\subsection{Walidacja krzyøowa}
Kolejnπ metodπ walidacji jest tzw. \textit{k}-krotna walidacja krzyøowa \textit{(ang. k-fold cross-validation)}. Tak samo jak w przypadku metody \textit{holdout} naleøy najpierw wydzieliÊ zbiÛr testowy, ktÛry nie bÍdzie bra≥ udzia≥u w procesie uczenia i walidacji. Pozosta≥e dane dzielone sπ na \textit{k} zbiorÛw, przewaønie jest to \textit{k = 5} lub \textit{k = 10}. NastÍpnie \textit{k}-krotnie powtarzany jest proces wyboru jednego \textit{i}-tego zbioru, gdzie \textit{i} oznacza kolejne liczby z przedzia≥u \textit{<1; k>}. ZbiÛr o numerze \textit{i} wybierany jest jako zbiÛr walidacyjny, natomiast reszta \textit{k - 1} zbiorÛw s≥uøπ jako dane uczπce. Naleøy pamiÍtaÊ, øeby w kaødym przebiegu uczyÊ nowπ instancjÍ modelu. W rezultacie model uczony jest \textit{k}-krotnie na rÛønych kombinacjach danych. Jako wynik koÒcowy brana jest pod uwagÍ úrednia skutecznoúÊ wszystkich modeli. SposÛb podzia≥u danych w tej metodzie ilustruje rysunek \ref{fig:cross-validation}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/cross-validation.pdf}
	\caption{Podzia≥ danych wed≥ug metody k-krotnej walidacji krzyøowe}
	\label{fig:cross-validation}
\end{figure}

Przedstawionπ metodÍ moøna rozszerzyÊ do jej iteracyjnej wersji polegajπcej na tym, øe ca≥y proces \textit{k}-krotnej walidacji krzyøowej wykonywany jest \textit{n} razy. Dodatkowo po kaødej iteracji moøna zastosowaÊ mieszanie danych. Walidacja tπ metodπ co prawda trwa o wiele d≥uøej, gdyø model walidowany jest \textit{k * n} razy, jednak pozwala uzyskaÊ bardziej wiarygodne wyniki, gdy nie jest dostÍpna zbyt duøa liczba danych.

\subsection{Implementacja walidacji}
\label{crossvalidation_implementation}
Zaimplementowane zosta≥y obie metody podzia≥u danych, jednak ze wzglÍdu na swoje zalety metody iteracyjnej \textit{k}-krotnej walidacji krzyøowej to w≥asnie ona zosta≥a uøyta do walidacji w procesie uczenie. Podzia≥ danych wed≥ug obu metod zosta≥ wykonany za pomocπ biblioteki \textit{sklearn}, ktÛra dostarcza przygotowane do tego celu funkcje (patrz listing \ref{lst:split}).
\begin{lstlisting}[caption=Podzia≥ danych na podzbiory treningowe i testowe, language=Python, label={lst:split}, float=h!]
from sklearn.model_selection import train_test_split, StratifiedKFold
from data_reader import get_data

def load_data_kfold(folds_number, test_size=0.05)):
    x, y = get_data(CHUNK_SIZE_IN_SECONDS)
    
    x_train, x_test, y_train, y_test = train_test_split(x, 
                                                        y, 
                                                        test_size=test_size)
    
    folds = list(StratifiedKFold(n_splits=folds_number, 
                                 shuffle=True, 
                                 random_state=1).split(x_train, y_train))
    
    return folds, x_train, y_train, x_test, y_test

def load_data_train_test(test_size=0.05):
    x, y = get_data(CHUNK_SIZE_IN_SECONDS)
    
    x_train, x_test, y_train, y_test = train_test_split(x, 
                                                        y, 
                                                        test_size=test_size)
    
    return x_train, y_train, x_test, y_test
\end{lstlisting}

Tak przygotowane dane podawane sπ do modelu w \textit{n} iteracjach za pomocπ prostej pÍtli. Wyniki z kaødej iteracji zapisywane sπ do tablicy, a na koniec obliczana jest ich úrednia. Proces iteracyjnego uruchamiania uczenia wraz z obliczaniem wynikÛw zaprezentowany zosta≥ na listingu \ref{lst:iteration_learning}

\begin{lstlisting}[caption=Iterecyjne wywo≥anie procesu uczenia, language=Python, label={lst:iteration_learning}]
number_of_iterations = 5

avg_accuracies = []
std_accuracies = []

for iteration in range(0, number_of_iterations):
    iteration_number = iteration + 1 
    print("Iteration", iteration_number)
    
    score, best_model_score = run_pipeline(create_model=model,
                                           folds=folds,
                                           x=x_train,
                                           y=y_train,
                                           epochs=100)

    accuracy = [row[1] for row in best_model_score]

    avg_accuracy = np.mean(accuracy)
    print("Best models average validation accuracy: {}".format(round(avg_accuracy, 6)))
    
    avg_accuracies.append(avg_accuracy)
   
grand_mean_avg = np.mean(avg_accuracies)
print("~~~Grand mean of average accuracy: {}".format(round(grand_mean_avg, 6)))
\end{lstlisting}

Do uruchamiania procesu uczenia uøywana jest funkcja \textit{run\_pipeline}, ktÛra szczegÛ≥owo opisana zostanie w rozdziale \ref{run_learning}.

\section{Uruchomienie uczenia w≥aúciwego}
\label{run_learning}
Do uruchomienia procesu uczenia w≥aúciwego s≥uøy funkcja \textit{run\_pipeline()}, ktÛra wykonuje nastÍpujπce czynnoúci:
\begin{itemize}
\item tworzy model na podstawie dostarczonych specyfikacji (\ref{build_model}),
\item pobiera odpowiednie porcje danych dla kaødego kroku z przytogowanych wczeúniej zbiorÛw (\ref{crossvalidation_implementation}),
\item tworzy listÍ tzw. \textit{callback}'Ûw (\ref{callbacks}),
\item uruchamia na modelu metodÍ rozpoczynajπcπ uczenie (\ref{fit}),
\item wczytuje wagi najlepszego modelu i sprawdza jego skutecznoúÊ (\ref{verify_accuracy}).
\end{itemize}

Kod metody \textit{run\_pipeline} zaprezentowany zosta≥ na listingu \ref{lst:run-pipeline}. PoszczegÛlne kroki zostanπ opisane w kolejnych rozdzia≥ach.

\begin{lstlisting}[caption=Implementacja metody uruchamiajπcej uczenie w≥aúciwe, language=Python, label={lst:run-pipeline}]
def run_pipeline(create_model, folds, x, y, epochs):
    best_model_score = []
    
    for fold_number, (train_idx, val_idx) in enumerate(folds):
        print('\nFold: ', fold_number)
        # wybÛr odpowiednich podzbiorÛw
        x_train_cv = x[train_idx]
        y_train_cv = y[train_idx]
        x_valid_cv = x[val_idx]
        y_valid_cv = y[val_idx]
                
        input_shape = x.shape[1:]

        # stworzenie modelu
        model, model_description = create_model(input_shape)

        # utworzenie callback'Ûw
        callbacks = callbacks_list("{}. Fold: {}.".format(model_description, 
                                                          fold_number))
        # rozpoczÍcie uczenia
        history = model.fit(x_train_cv,
                            y_train_cv,
                            epochs=epochs,
                            batch_size=16,
                            callbacks=callbacks,
                            validation_data=(x_valid_cv, y_valid_cv),
                            verbose=0)

        # wczytanie wag najlepszego modelu i sprawdzenie jego skutecznoúci
        model.load_weights("tmp/best_model.h5")
        best_model_score.append(model.evaluate(x_valid_cv, y_valid_cv, batch_size=16, verbose=0))
        print("--Best model validation accuracy: %.2f%%" % (best_model_score[fold_number][1]*100))
        
    return best_model_score
\end{lstlisting}

\subsection{Budowa modelu}
\label{build_model}
Model tworzony jest na podstawie dostarczonych specyfikacji w postaci funkcji. Funkcja wykonuje kolejne kroki budujπce model z kolejnych warstw, ktÛre dostÍpne sπ jako gotowe komponenty z biblioteki Keras. Kaøda z warstw jest parametryzowana wybranymi wartoúciami. 
Po utworzeniu modelu jest on kompilowany. Na tym etapie podawane sπ rÛwnieø informacje odnoúnie tego jaki optymalizator, funkcja strat oraz metryka skutecznoúci ma zostaÊ uøyta. Dodatkowo tworzony jest rÛwnieø opis modelu wykorzystywany pÛüniej w procesie monitorowania.

Na listingu \ref{lst:model-example} zosta≥a zaprezentowana funkcja tworzπca przyk≥adowy model splotowej sieci neuronowej z 1-wymiarowym filtrem.

\begin{lstlisting}[caption=Tworzenie przyk≥adowego modelu, language=Python, label={lst:model-example}]
def conv_1D_with_adam(input_shape):
    # opis modelu tworzony na podstawie nazwy funkcji
    description = get_function_name()
    
    # specyfikacja modelu tworzonego sekwencyjnie
    model = Sequential()

    # dodanie warstwy splotowej z 32 1-wymiarowymi filtrami o rozmiarze 6 z funkcjπ aktywacji 'relu'
    model.add(Conv1D(filters=32, kernel_size=6, padding='same', activation='relu', input_shape=input_shape))
    # warstwa max pooling'u o rozmiarze 2
    model.add(MaxPooling1D(pool_size=2))

    # warstwa zmniejszajπca liczbÍ wymiarÛw danych
    model.add(Flatten())
    # warstwa typu 'fully-connected' o rozmiarze 64 neuronÛw
    model.add(Dense(64, activation='relu'))
    # warstwa typu 'fully-connected' z jednym neuronem i sigmoidalnπ funkcjπ aktywacji, ktÛra na wyjúciu zwrÛci wartoúÊ '0' lub '1'
    model.add(Dense(1, activation='sigmoid'))

    # kompilacja modelu z uøyciem optymalizatora Adam, funkcji strat binarnej entropii krzyøowej oraz dok≥adnoúci (accuracy) jako metryki
    model.compile(optimizer=Adam(),                  
                  loss='binary_crossentropy',
                  metrics=['acc'])
 
    return model, description
\end{lstlisting}

\subsection{Lista callback'Ûw}
\label{callbacks}
Dodatkowym parametrem przekazywanym do procesu uczenia jest lista tzw. \textit{callback}'Ûw, czyli wywo≥aÒ zwrotnych, ktÛre umoøliwiajπ otrzymywanie informacji z wnÍtrza modelu podczas jego nauki.

Zosta≥y utworzone 4 nastÍpujπce callbacki:
\begin{itemize}
\item \textbf{EarlyStopping} - pozwala na wczeúniejsze zatrzymanie procesu uczenia w przypadku, gdy monitorowana metryka nie zosta≥a poprawiona przez okreúlonπ iloúÊ epok,
\item \textbf{LearningRateScheduler} - implementuje adaptacyjnπ metodÍ zmiany wspÛ≥czynnika uczenia wed≥ug podanych regu≥,
\item \textbf{ModelCheckpoint} - umoøliwia zapisanie modelu, ktÛry posiada najlepsze dopasowanie wed≥ug okreúlonej metryki,
\item \textbf{TensorBoard} - tworzy logi z procesu uczenia w podanym folderze, ktÛre mogπ byÊ uøyte do monitorowania przez narzÍdzie TensorBoard \ref{monitoring}.
\end{itemize}


Listing \ref{lst:callback} przedstawia implementacjÍ metod tworzπcych \textit{callback}'i.

\begin{lstlisting}[caption=Tworzenie listy \textit{callback}'Ûw, language=Python, label={lst:callback}]
def step_decay(epoch):    
    initial_lrate=0.1
    drop=0.6
    epochs_drop = 10.0
    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
    
    return lrate


def callbacks_list(description): 
    return [
    callbacks.EarlyStopping(
        monitor='val_acc', 
        patience=30
    ),
    callbacks.LearningRateScheduler(step_decay),
        
    callbacks.ModelCheckpoint(
        filepath='tmp/best_model.h5', 
        monitor='val_acc',
        save_best_only=True
    ),
    callbacks.TensorBoard(
        log_dir='tmp/logs/{}. {}'.format(description, create_current_time()),
        histogram_freq=0,
        write_graph=True,
        write_images=True
    )
]
\end{lstlisting}

\subsection{Funkcja dopasowania}
\label{fit}
Wykonywana na modelu funkcja \textit{fit()} rozpoczyna procedurÍ uczenia, czyli dopasowania do danych.
Jako argumenty przykazywane sπ do niej dane treningowe i walidacyjne, liczba epok uczπcych, \textit{callback}'i oraz inne parametry. Podczas nauki wyúwietlane sπ informacje o postÍpie uczenia oraz zwracany jest raport zawierajπcy historiÍ wartoúci metryk w kolejnych epokach, na podstawie ktÛrych moøna np. narysowaÊ wykres przedstawiajπcy postÍpy.
Te wartoúci nie bÍdπ jednak uøywane, gdyø ca≥a historia i postÍpy uczenia sπ ≥adowane do programu monitorujπcego TensorBoard z logÛw tworzonych przez callback TensorBoard (patrz rozdzia≥ \ref{monitoring}).
Dodatkowo dla kaødej iteracji zapisywany jest najlepszy model, ktÛry na sam koniec zostanie za≥adowany i sprawdzony pod kπtem skutecznoúci (patrz listing \ref{verify_accuracy}).

\begin{lstlisting}[caption=Wywo≥anie metody \textit{fit()} na modelu, language=Python, label={lst:fit}]
history = model.fit(x_train_cv,
                    y_train_cv,
                    epochs=epochs,
                    batch_size=16,
                    callbacks=callbacks,
                    validation_data=(x_valid_cv, y_valid_cv),
                    verbose=0)
\end{lstlisting}

\subsection{Sprawdzenie skutecznoúci}
\label{verify_accuracy}
W celu sprawdzenia skutecznoúci wczytywany jest najlepiej dopasowany model z danej iteracji i jest on sprawdzany metodπ \textit{evaluate()} na danych walidacyjnych. Wynik jest nastÍpnie wyúwietlany i zapisywany do tablicy, ktÛra pos≥uøy do policzenia úredniej skutecznoúci modelu. Przedstawione dzia≥ania wykonuje kod zaprezentowany na listingu \ref{lst:validation}

\begin{lstlisting}[caption=Sprawdzenie skutecznoúci najlepszego modelu, language=Python, label={lst:validation}]
model.load_weights("tmp/best_model.h5")
best_model_score.append(model.evaluate(x_valid_cv, y_valid_cv, batch_size=16, verbose=0))
        
print("--Best model validation accuracy: %.2f%%" % (best_model_score[fold_number][1]*100))
\end{lstlisting}


\section{Monitorowanie}
\label{monitoring}
Model sieci neuronowej po zakoÒczeniu procesu nauki zwraca obiekt zawierajπcy raport z wynikami. W sytuacji, gdy proces uczenia przeprowadzany jest kilkukrotnie naleøa≥oby rÍcznie zarzπdzaÊ obiektami raportÛw. Dodatkowo, przedstawienie wynikÛw w sposÛb czytelny i ≥atwy do zinterpretowania wymaga zaimplementowania metod wizualizacyjnych np. rysowanie wykresu. Nie ma jednak potrzeby rÍcznej implementacji wyøej wymienionych funkcjonalnoúci, gdyø istniejπ dedykowane narzÍdzia umoøliwiajπce prezentacjÍ wynikÛw w czasie rzeczywistym w przyjaznej dla uøytkownika formie.

Do monitorowania procesu uczenia zosta≥o uøyte narzÍdzie \textit{TensorBoard}, ktÛre dostarczone jest razem z bibliotekπ \textit{TensorFlow}. Pozwala ono m.in. na graficznπ wizualizacjÍ postÍpÛw w procesie uczenia oraz automatyczne generowanie schematu modelu sieci neuronowej. Jest to narzÍdzie uruchamiane w przeglπdarce, ktÛre wykorzystuje logi wygenerowane przez \textit{callback.TensorBoard} omawiany w rozdziale \ref{callbacks}. 

Na podstawowym widoku przedstawione sπ 4 wykresy ilustrujπce nastÍpujπce wartoúci:
\begin{itemize}
\item \textbf{acc} - dok≥adnoúÊ w procesie uczenia,
\item \textbf{loss} - wartoúÊ funkcji strat w procesie uczenia,
\item \textbf{val\_acc} - dok≥adnoúÊ w procesie walidacji,
\item \textbf{val\_loss} - wartoúÊ funkcji strat w procesie walidacji.
\end{itemize}\
Wykresy zosta≥y zaprezentowane na rysunku \ref{fig:tensor-board}. Sπ one interaktywe, wiÍc moøna sprawdziÊ dok≥adnπ wartoúÊ w kaødym punkcie wykresu.
\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/tensor-board.png}
	\caption{Dynamicznie tworzone wykresy w programie TensorBoard}
	\label{fig:tensor-board}
\end{figure}

Wizualna reprezentacja utworzonego modelu rÛwnieø moøe byÊ pomocna przy ocenie architektury tworzonej sieci neuronowej. Fragment przyk≥adowego schematu wygenerowany przez TensorBoard na podstawie modelu utworzonego w kodzie pokazany zosta≥ na rysunku \ref{fig:tensor-board-model}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=10cm]{Rysunki/Rozdzial5/tensor-board-model.png}
	\caption{Fragment schematu modelu wygenerowany przez TensorBoard}
	\label{fig:tensor-board-model}
\end{figure}

TensorBoard udostÍpnia rÛwnieø wiele innych funkcjonalnoúci, ktÛre nie zosta≥y uøyte jak np. rysowanie wykresÛw w≥asnorÍcznie zdefiniowanych metryk, monitorowanie i wizualizacjÍ rozk≥adu danych w grafie itp. WiÍcej informacji na temat moøliwoúci TensorBoard moøna znaleüÊ w dokumentacji \cite{tensor_board}. 

\section{WybÛr rodzaju sieci neuronowej}
WybÛr rodzaju oraz wstÍpnej architektury sieci neuronowej ma kluczowe znaczenie. To w≥aúnie te czynniki bÍdπ w g≥Ûwnej mierze decydowaÊ o skutecznoúci modelu. Po dobraniu rodzaju sieci neuronowej, ktÛry umoøliwia uzyskanie jak najlepszych wynikÛw nastÍpuje faza optymalizacji, czyli dostosowywania parametrÛw, ktÛra zostanie opisana w rozdziale \ref{optimization}.

Przy wyborze modelu warto zaczπÊ od rozwiπzaÒ najprostszych. Proste modele bÍdπ potrzebowa≥y zdecydowanie mniej czasu na naukÍ, niø te z≥oøone, dlatego dziÍki uøyciu tego podejúcia bÍdzie moøna w krÛtkim czasie uzyskaÊ pierwsze rezultaty. Moøe siÍ rÛwnieø okazaÊ, øe bardzo prosty model jest dostatecznie skuteczny w aktualnie rozpatrywanym problemie.

Podczas prÛby znalezienia odpowiedniego rodzaju sieci neuronowej rozpatrzone zosta≥y nastÍpujπce opcje:
\begin{itemize}
\item klasyczna sieÊ typu \textit{fully-connected},
\item sieÊ splotowa z filtrem 1-wymiarowym,
\item sieÊ splotowa z filtrem 2-wymiarowym,
\item sieÊ rekurencyjna,
\item po≥πczenie sieci splotowej z rekurencyjnπ.
\end{itemize}

\subsection{Klasyczna sieÊ neuronowa typu fully connected}
Pierwszym, najprostszym rozwiπzaniem, ktÛre zosta≥o przetestowane jest klasyczna sieÊ neuronowa typu \textit{fully connected}. W bibliotece Keras warstwy tworzπce takπ sieÊ noszπ nazwÍ \textit{Dense}.
Model poczπtkowy, ktÛry zosta≥ stworzony zaprezentowano na listingu \ref{lst:fully-connected}. 

\begin{lstlisting}[caption=Model sieci neuronowej typu \textit{fully connected}, language=Python, label={lst:fully-connected}]
model = Sequential()
model.add(Flatten(input_shape=input_shape))
model.add(Dense(1000, kernel_initializer='normal', activation='relu'))
model.add(Dense(30, kernel_initializer='normal', activation='relu'))
model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))
    
sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)
model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])
\end{lstlisting}

SieÊ sk≥ada siÍ z warstwy wejúciowej \textit{Flatten}, ktÛra odpowiada za zmniejszenie stopnia wymiarowoúci danych tak, aby mog≥y byÊ uøyte w sieci tego typu. Przez tÍ operacjÍ tracone sπ pewne informacje o sπsiedztwie danych, gdyø teraz sπ one jedynie wektorem zamiast wielowymiarowπ tablicπ, jednak jest ona wymagana.
Kolejne warstwy stanowiπ warstwy \textit{Dense} zawierajπce rÛønπ iloúÊ neuronÛw. Poczπtkowo zosta≥y uøyte wartoúci 1000 i 30.
SieÊ zakoÒczona jest warstwπ z jednym neuronem i sigmoidalnπ funkcjπ aktywacji, ktÛra zwraca na wyjúciu wartoúÊ 1 lub 0.
Jako optymalizator uøyty zosta≥ \textit{SGD} (\textit{ang. stochastic gradient descent}) z funkcjπ strat binarnej entropii krzyøowej \textit{(ang. binary crossentropy)}. Podsumowanie modelu wygenerowane przez bibliotekÍ \textit{Keras}, opisujπce iloúÊ warstw oraz ich wielkoúci, zamieszczono na listingu \ref{lst:summary_fc}.

\begin{lstlisting}[caption=Podsumowanie modelu sieci neuronowej typu \textit{fully connected}, language=Python, label={lst:summary_fc}]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_3 (Flatten)          (None, 3200)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 1000)              3201000   
_________________________________________________________________
dense_4 (Dense)              (None, 30)                30030     
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 31        
=================================================================
Total params: 3,231,061
Trainable params: 3,231,061
Non-trainable params: 0
____________________________
\end{lstlisting}

Proces nauki zaproponowanej sieci neuronowej przebiega≥ szybko ze wzglÍdu na jej prostotÍ. Jedna pe≥na iteracja trwa≥a jedynie oko≥o 90 sekund. Wyniki by≥y jednak bardzo s≥abe. SkutecznoúÊ podczas uczenia modelu oscylowa≥a wokÛ≥ wartoúci 50\% (48\% - 52\%). SkutecznoúÊ walidacyjna rÛwnieø by≥a praktycznie sta≥a i zawsze bliska \textbf{50\%} \ref{fig:fully-connected-results}. 

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/fully-connected-results.png}
	\caption{Wyniki skutecznoúci sieci typu \textit{fully-connected}}
	\label{fig:fully-connected-results}
\end{figure}

Moøna wiÍc zauwaøyÊ, øe sieÊ dzia≥a≥a praktycznie w sposÛb losowy, gdyø tyle wynosi w≥aúnie prawdopodobieÒstwo przewidzenia wyniku z poúrÛd dwÛch wartoúci. Pomimo prÛby modyfikacji sieci poprzez zwiÍkszenie/zmniejsznie liczby warstw, zwiÍkszenie/zmniejszenie liczby neuronÛw, zmiany optymalizatora i funkcji strat, wciπø przynosi≥a ona podobne wyniki. Nie uda≥o siÍ osiπgnπÊ lepszych rezultatÛw dla tego typu sieci neuronowej, dlatego nie by≥a ona wiÍcej wykorzystywana. Okaza≥a siÍ niewystarczajπca do rozwiπzania rozpatrywanego problemu.


\subsection{SieÊ splotowa z filtrem 1-wymiarowym}
\label{conv-1d}
Kolejnym testowanym typem by≥a sieÊ splotowa z 1-wymiarowym filtrem, ktÛrej zastosowanie polecane jest do danych w formie szeregÛw czasowych np. dane z øyroskopu czy akcelerometru. W bibliotece Keras warstwy tego typu nazwane sπ \textit{Conv1D (ang. Convolutional 1-dimension)}. Filtr jednowymiarowy jest w stanie odnajdywaÊ podobieÒstwa miÍdzy danymi jedynie w jednym wymiarze. W przypadku dostÍpnych danych z EEG mogπ byÊ to wartoúci tylko z jednego kana≥u jednoczeúnie. Oznacza to, øe byÊ moøe sieÊ tego typu nie bÍdzie w stanie zauwaøyÊ zaleønoúci pomiÍdzy danymi znajdujπcymi siÍ na rÛønych kana≥ach.

WstÍpny model sieci, ktÛry zosta≥ wybrany przedstawiony zosta≥ na listingu \ref{lst:conv-1d}. Sk≥ada siÍ on z dwÛch warstw konwolucyjnych o odpowiednio 32 i 16 filtrach z warstwami \textit{max pooling}'u. Sieci splotowe zwykle zakoÒczone sπ kilkoma warstwami typu \textit{fully-connected}. W tym wypadku zosta≥a zastosowana jedna warstwa z 64 neuronami oraz warstwa wyjúciowa z jednym neuronem i sigmoidalnπ funkcjπ aktywacji. Warstwa koÒcowa tego typu bÍdzie wykorzystywana w kaødym modelu, gdyø daje ona moøliwoúÊ zwrÛcenia na wyjúciu z sieci wartoúÊ 0 lub 1, ktÛre oznaczajπ brak lub pojawienie siÍ ataku. Podsumowanie modelu znajduje siÍ na listingu \ref{lst:conv1d_summary}

\begin{lstlisting}[caption=Model splotowej sieci neuronowej z 1-wymiarowym filtrem, language=Python, label={lst:conv-1d}]
model = Sequential()

model.add(Conv1D(filters=32, kernel_size=6, padding='same', activation='relu', input_shape=input_shape))
model.add(MaxPooling1D(pool_size=2))
    
model.add(Conv1D(filters=16, kernel_size=6, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
    
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['acc'])
\end{lstlisting}

\begin{lstlisting}[caption=Podsumowanie modelu splotowej sieci neuronowej z 1-wymiarowym filtrem, language=Python, label={lst:conv1d_summary}]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_2 (Conv1D)            (None, 200, 32)           3104      
_________________________________________________________________
dropout_5 (Dropout)          (None, 200, 32)           0         
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 100, 32)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 100, 16)           3088      
_________________________________________________________________
dropout_6 (Dropout)          (None, 100, 16)           0         
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 50, 16)            0         
_________________________________________________________________
flatten_5 (Flatten)          (None, 800)               0         
_________________________________________________________________
dense_9 (Dense)              (None, 64)                51264     
_________________________________________________________________
dense_10 (Dense)             (None, 1)                 65        
=================================================================
Total params: 57,521
Trainable params: 57,521
Non-trainable params: 0
__________________________
\end{lstlisting}

Skonstruowana w ten sposÛb sieÊ neuronowa uczy≥a siÍ co prawda d≥uøej, niø sieÊ typu \textit{fully-connected}, gdyø czas nauki wynosi≥ oko≥o 240 sekund, jednak wyniki by≥y o wiele lepsze i oscylowa≥y w okolicach \textbf{70\%}. Jest to bardzo dobry wynik w porÛwnaniu do poprzedniego biorπc pod uwagÍ prostπ architekturÍ sieci. SieÊ jednak bardzo szybko siÍ przeucza≥a \textit{(ang. overfitting)}. Moøna to zauwaøyÊ po osiπgniÍciu 100\% skutecznoúci podczas procesu nauki \textit{(acc)} oraz wzroúcie wartoúci funkcji strat podczas walidacji \textit{(val\_loss)} (patrz rysunek \ref{fig:conv-1d-results}).

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/conv-1d-results.png}
	\caption{Wyniki prostej sieci splotowej z 1-wymiarowym filtrem}
	\label{fig:conv-1d-results}
\end{figure}

Wyniki by≥y obiecujπce, wiÍc sieÊ tego typu zosta≥a wziÍta pod uwagÍ w dalszych pracach polegajπcych na optymalizacji w celu uzyskania jak najlepszych wynikÛw (rozdzia≥ \ref{optimization}).

\subsection{SieÊ splotowa z filtrem 2-wymiarowym}
NastÍpnym typem sieci, ktÛra zosta≥a przetestowana by≥a sieÊ splotowa z filtrem 2-wymiarowym \textit{(Conv2D)}. SieÊ ta jest bardzo podobna do konwolucyjnej sieci jednowymiarowej opisywanej w poprzednim podrozdziale \ref{conv-1d}. RÛønicπ jest typ zastosowanego filtra, ktÛry w tym przypadku jest 2-wymiarowy. Pozwala wiÍc analizowaÊ dane w dwuwymiarowej przestrzeni. Sieci tego typu uøywane sπ przewaønie do rozpoznawania obrazÛw, jednak rÛwnie dobrze mogπ siÍ sprawdziÊ teø na danych innego typu. W przypadku EEG sieÊ z 2-wymiarowym filtrem bÍdzie w stanie analizowaÊ szereg czasowy obejmujπc kilka kana≥Ûw jednoczeúnie, dziÍki temu moøliwe bÍdzie zauwaøenie powiπzania pomiÍdzy danymi z innych kana≥Ûw.

Analogicznie do modelu sieci z filtrem jednowymiarowym zosta≥ stworzony podobny z 2-wymiarowym filtrem (patrz listing \ref{lst:conv-2d}).
Podsumowanie modelu zaprezentowano na listingu \ref{lst:conv2d_summary}.

\begin{lstlisting}[caption=Model splotowej sieci neuronowej z 2-wymiarowym filtrem, language=Python, label={lst:conv-2d}, float=!h]
model = Sequential()

model.add(Conv2D(32,(3,3),strides = (1,1),padding='same', activation = 'relu', input_shape=input_shape))
model.add(MaxPooling2D((2,2)))
    
model.add(Conv2D(16,(3,3),strides = (1,1),name='conv3', activation = 'relu'))
model.add(MaxPooling2D((2,2)))
    
model.add(Flatten())
model.add(Dense(64,activation = 'relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['acc'])
\end{lstlisting}

\begin{lstlisting}[caption=Podsumowanie modelu splotowej sieci neuronowej z 2-wymiarowym filtrem, language=Python, label={lst:conv2d_summary}, float=!h]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 200, 16, 32)       320       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 100, 8, 32)        0         
_________________________________________________________________
conv3 (Conv2D)               (None, 98, 6, 16)         4624      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 49, 3, 16)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 2352)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                150592    
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 155,601
Trainable params: 155,601
Non-trainable params: 0
_________________________________________________________________
\end{lstlisting}

Nauka tego modelu sieci trwa≥a trochÍ d≥uøej, niø poprzedniego, gdyø czas wynosi≥ oko≥o 300 sekund. Okaza≥o siÍ jednak, øe sieÊ ktÛra rÛøni siÍ od poprzedniej jedynie wymiarem filtra osiπgnÍ≥a skutecznoúÊ na poziomie \textbf{75\%}. 
Po wynikach zaprezentowanych na rysunku \ref{fig:conv-2d-results} moøna jednak zauwaøyÊ, øe pomimo lepszej skutecznoúci sieÊ tego typu rÛwnieø bardzo szybko ulega przeuczeniu. Problem ten bÍdzie rozwiπzywany podczas prÛby jej optymalizacji w rozdziale \ref{optimization}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/conv-2d-results.png}
	\caption{Wyniki prostej sieci splotowej z 2-wymiarowym filtrem}
	\label{fig:conv-2d-results}
\end{figure}

\subsection{SieÊ rekurencyjna}
W nastÍpnym kroku przetestowana zosta≥a rekurencyjna sieÊ neuronowa. Poczπtkowy model sieci, ktÛry zosta≥ utworzony zaprezentowany zosta≥ na listingu \ref{lst:recursive}. Podsumowanie modelu zamieszczono na listingu \ref{lst:rec_summary}.

\begin{lstlisting}[caption=Model rekurencyjnej sieci neuronowej, language=Python, label={lst:recursive}, float=!h]
model = Sequential()

model.add(LSTM(100, input_shape=input_shape))
model.add(Dense(1, activation='sigmoid'))
    
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
\end{lstlisting}

\begin{lstlisting}[caption=Podsumowanie modelu rekurencyjnej sieci neuronowej, language=Python, label={lst:rec_summary}, float=!h]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_2 (LSTM)                (None, 100)               46800     
_________________________________________________________________
dense_17 (Dense)             (None, 1)                 101       
=================================================================
Total params: 46,901
Trainable params: 46,901
Non-trainable params: 0
_________________________________________________________________
\end{lstlisting}

Sk≥ada siÍ ona z jednej warstwy typu \textit{LSTM (ang. Long Short-Term Memory)}. SieÊ mimo prostej budowy uczy siÍ bardzo d≥ugo. Czas pe≥nej iteracji nauki wynosi aø 5700 sekund. Wynika to ze z≥oøonoúci sposobu funkcjonowania tego typu sieci. Pomimo d≥uøszego czasu nauki zaproponowana sieÊ osiπga skutecznoúÊ na poziomie jedynie ok. \textbf{60\%} \ref{fig:recursive-results}. 

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/recursive-results.png}
	\caption{Wyniki prostej sieci rekurencyjnej}
	\label{fig:recursive-results}
\end{figure}

Jest to wynik o wiele gorszy w porÛwnaniu do sieci konwolucyjnych testowanych w poprzednich rozdzia≥ach. PrÛby rozbudowy i prostej optymalizacji sieci nie przynosi≥y poøπdanych skutkÛw, dlatego zosta≥a ona pominiÍta w dalszych badaniach.
 
Poczyniono jednak prÛby po≥πczenia tego typu sieci z sieciami konwolucyjnymi. Wyniki przeprowadzonych prÛb opisane zosta≥y w rozdziale \ref{cnn_lstm}

\subsection{Po≥πczenie sieci splotowej z rekurencyjnπ}
\label{cnn_lstm}
Ostatnim typem sieci, ktÛrego skutecznoúÊ zosta≥a sprawdzona jest po≥πczenie sieci splotowej z rekurencyjnπ tzw. \textit{CNN LSTM}. Jest to bardziej rozbudowana stuktura sieci polegajπca na uøyciu kilku warstw konwolucyjnych zakoÒczonych warstwami rekurencyjnymi. Prosty model sieci zbudowany wed≥ug tej koncepcji zaprezentowany zosta≥ na listingu \ref{lst:cnn_lstm}. Jego podsumowanie zamieszczono na listingu \ref{lst:cnn_lstm_summary}.

\begin{lstlisting}[caption=Model sieci typu CNN LSTM, language=Python, label={lst:cnn_lstm}, float=!h]
model = Sequential()

model.add(Conv1D(filters=64, kernel_size=6, padding='same', activation='relu', input_shape=input_shape))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(filters=32, kernel_size=6, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
    
model.add(LSTM(100))
    
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
    
model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['acc'])
\end{lstlisting}

\begin{lstlisting}[caption=Podsumowanie modelu sieci typu CNN LSTM, language=Python, label={lst:cnn_lstm_summary}, float=!h]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_2 (Conv1D)            (None, 200, 32)           3104      
_________________________________________________________________
dropout_1 (Dropout)          (None, 200, 32)           0         
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 100, 32)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 100, 16)           3088      
_________________________________________________________________
dropout_2 (Dropout)          (None, 100, 16)           0         
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 50, 16)            0         
_________________________________________________________________
lstm_4 (LSTM)                (None, 100)               46800     
_________________________________________________________________
dense_19 (Dense)             (None, 128)               12928     
_________________________________________________________________
dense_20 (Dense)             (None, 1)                 129       
=================================================================
Total params: 66,049
Trainable params: 66,049
Non-trainable params: 0
_________________________________________________________________
\end{lstlisting}

Pomimo bardziej z≥oøonej struktury proces nauki utworzonej sieci przebiega≥ krÛcej, niø w przypadku tej sk≥adajπcej siÍ jedynie z warstwy rekurencyjnej i wynosi≥ oko≥o 1800 sekund. Spowodowane jest to tym, øe dane wejúciowe ulegajπ zmniejszeniu za pomocπ warstw konwolucyjnych i wstÍpnie przetworzone trafiajπ na warstwÍ rekurencyjnπ. DziÍki temu warstwa rekurencyjna potrzebuje mniej czasu do nauki, niø w przypadku, gdy podawane jej sπ nieprzygotowane dane.

OsiπgniÍte wyniki by≥y co prawda lepsze od sieci rekurencyjnej, jednak nie przewyøsza≥y wynikÛw modeli zbudowanych z uøyciem warstw konwolucyjnych. OsiπgniÍte wyniki waha≥y siÍ w granicach \textbf{65\%} \ref{fig:cnn_lstm}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/cnn-lstm-results.png}
	\caption{Wyniki sieci konwoluncyjnej po≥πczonej z rekurencyjnπ}
	\label{fig:cnn_lstm}
\end{figure}
Zosta≥y podjÍte prÛby roszerzenia modelu sieci oraz zmiany parametrÛw, jednak wyniki nie zosta≥y w znaczπcym stopniu poprawione. Ze wzglÍdu na to sieÊ tego typu nie zosta≥a uøyta w dalszych badaniach.


\subsection{Podsumowanie}
Podsumowanie wynikÛw testowania skutecznoúci poszczegÛlnych typÛw sieci zosta≥y zaprezentowane w tabeli \ref{tbl:comparison}. 

\begin{table}[h!]
	\caption{PorÛwnanie czasÛw nauki oraz skutecznoúci poszczegÛlnych typÛw sieci}
	\label{tbl:comparison}
	\begin{tabular}{|l|l|l|}
		\hline
		\textbf{Nazwa}  & \textbf{åredni czas jednej iteracji {[}s{]}} & \textbf{årednia skutecznoúÊ {[}\%{]}} \\ \hline
		Fully connected & 90                                           & 50                                    \\ \hline
		CNN 1D          & 240                                          & 70                                    \\ \hline
		CNN 2D          & 300                                          & 75                                    \\ \hline
		LSTM            & 5700                                         & 60                                  \\ \hline
		CNN LSTM        & 1800                                         & 65                                  \\ \hline
	\end{tabular}
\end{table}

Moøna zauwaøyÊ, øe najlepsze wyniki i stosunkowo nieduøe czasy nauki osiπgnÍ≥a sieÊ splotowa z filtrem 2-wymiarowym. Model tej sieci zosta≥ wiÍc wybrany jako ten najlepiej przystosowany do rozwiπzywania badanego problemu. PrÛba i sposoby optymalizacji wynikÛw tego modelu zosta≥y opisane w kolejnym rozdziale \ref{optimization}.


\section{Optymalizacja}
\label{optimization}
SieÊ typu konwolucyjnego z filtrem 2-wymiarowym osiπga≥a najlepsze wyniki wúrÛd testowanych prototypÛw. To w≥aúnie tej architekturze zosta≥o wiÍc poúwiÍcone najwiÍcej pracy majπcej na celu optymalizacjÍ i polepszenie otrzymywanych wynikÛw. 
Wyniki juø nawet przy zaprezentowanym prostym modelu by≥y obiecujπce, gdyø sieÊ osiπga≥a úrednio 75\% skutecznoúci.

NajwiÍkszym i najczÍúciej wystÍpujπcym problemem podczas nauki sieci neuronowych jest przeuczenie \textit{(ang. overfitting)}, ktÛre polega na zbytnim dopasowaniu siÍ modelu do danych uczπcych. Przeuczony model bÍdzie posiada≥ zbyt ma≥π umiejÍtnoúÊ generalizacji, w wyniku czego bÍdzie bardzo dobrze potrafi≥ klasyfikowaÊ dane, ktÛre do tej pory widzia≥, jednak nie poradzi sobie zbyt dobrze z zupe≥nie nowymi.
Overfitting zwykle objawia siÍ osiπgniÍciem skutecznoúci bliskiej 100\% i wartoúci funkcji strat wynoszπcej oko≥o 0 podczas procesu uczenia. Skutkiem tego sπ oczywiúcie o wiele gorsze wyniki podczas procesu walidacji. Tak jest rÛwnieø w przypadku tego prostego modelu, co moøna zaobserwowaÊ na wykresach prezentujπcych przebieg uczenia przedstawiony na rysunku \ref{fig:cnn_start}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/cnn-start.png}
	\caption{Wyniki modelu poczπtkowego konwolucyjnej sieci neuronowej}
	\label{fig:cnn_start}
\end{figure}

Istnieje kilka sposobÛw przeciwdzia≥ania nadmiernego dopasowania. Zostanπ one opisane w kolejnych rozdzia≥ach.

\subsection{Batch normalization}
Pierwszym sposobem jest wykorzystanie \textit{batch normalization}. Technika ta polega na normalizacji wartoúci wyjúciowych z danej warstwy tak, aby mia≥y one úredniπ 0 i odchylenie standardowe 1. Jest to zabieg podobny do tego przeprowadzonego podczas przygotowywania danych do procesu nauki z tπ rÛønicπ, øe moøna go stosowaÊ dla wartoúci wyjúciowych z warstw. Dodatkowo zmniejsza zaleønoúÊ wynikÛw osiπganych przez sieÊ od wartoúci, ktÛrymi zainicjalizowane by≥y wagi oraz poprawia przep≥yw gradientu. WiÍcej informacji dotyczπcej \textit{batch normalization} moøna znaleüÊ w publikacji poúwiÍconej temu zagadnieniu \cite{batch_normalization}.

W bibliotece \textit{Keras} dostÍpne sπ gotowe warstwy o nazwie \textit{BatchNormalization} dostarczajπce opisanπ funkcjonalnoúÊ. Wystarczy dodaÊ warstwÍ poprzedzajπcπ funkcjÍ aktywacji \ref{lst:cnn_batch_normalization}.

\begin{lstlisting}[caption=Model sieci konwolucyjnej z warstwami BatchNormalization, language=Python, label={lst:cnn_batch_normalization}]
model = Sequential()

model.add(Conv2D(32, (3,3), strides = (1,1), padding='same', input_shape=input_shape))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(16, (3,3), strides = (1,1), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D((2,2)))

model.add(Flatten())
model.add(Dense(64, activation = 'relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['acc'])
\end{lstlisting}

Wykonanie tak prostej operacji pozwoli≥o na zwiÍkszenie úredniej skutecznoúci sieci do \textbf{77\%}. Problem \textit{overfitting}'u co prawda nie zosta≥ ca≥kowicie rozwiπzany, jednak moøna zauwaøyÊ drobne postÍpy wzglÍdem poprzedniej wersji modelu \ref{fig:cnn_batch_normalization}

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/cnn-batch-normalization.png}
	\caption{Wyniki modelu konwolucyjnej sieci neuronowej z warstwami \textit{BatchNormalization}}
	\label{fig:cnn_batch_normalization}
\end{figure}

\subsection{Dropout}
Kolejnπ technikπ jest wykorzystanie tzw. \textit{dropout}'u. Polega ona na tym, øe wartoúci na warstwie wyjúciowej sπ zerowane ze wskazanym prawdopodobieÒstwem. DziÍki temu podczas kaødej epoki uczπcej dane wyglπdajπ trochÍ inaczej, niø poprzednio, dlatego sieÊ nie uczy siÍ zawsze na podstawie tych samych danych, tylko ich drobnych modyfikacjach. W bibliotece \textit{Keras} zosta≥a zaimplementowana gotowa warstwa \textit{Dropout}, ktÛra umoøliwia wykorzystanie tej techniki \ref{lst:cnn_dropout}. Zosta≥a ona na≥oøona na wyniki warstwy typu \textit{Dense}.

\begin{lstlisting}[caption=Model sieci konwolucyjnej z warstwami Dropout, language=Python, label={lst:cnn_dropout}]
model = Sequential()

model.add(Conv2D(32, (3,3), strides = (1,1), padding='same', input_shape=input_shape))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(16, (3,3), strides = (1,1), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D((2,2)))

model.add(Flatten())
model.add(Dense(64, activation = 'relu'))
model.add(Dropout(0.25))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['acc'])
\end{lstlisting}

Po dodaniu \textit{dropout}'u wyniki nieznacznie siÍ poprawi≥y do wartoúci úrednio \textbf{78\%} oraz zmniejszony zosta≥ problem przeuczenia. Podczas uczenia nie jest osiπgana juø skutecznoúÊ 100\% oraz wartoúci funkcji strat rÛwnej 0. Podczas walidacji wartoúÊ funkcji strat dalej roúnie, jednak w mniejszym stopniu, niø poprzednio \ref{fig:cnn_dropout}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial5/cnn-dropout.png}
	\caption{Wyniki modelu konwolucyjnej sieci neuronowej z warstwami \textit{Dropout}}
	\label{fig:cnn_dropout}
\end{figure}

\subsection{WybÛr optymalizatora}
Kolejnym czynnikiem wp≥ywajπcym na skutecznoúÊ sieci jest wybrany optymalizator. Jego zadaniem jest modyfikacja wag na podstawie wartoúci funkcji strat.
W bibliotece \textit{Keras} istnieje wiele gotowych optymalizatorÛw, ktÛre mogπ zostaÊ wybrane podczas kompilacji modelu. Najbardziej popularnymi sπ: 
\textit{Adam}, \textit{RMSProp} oraz \textit{SGD (ang. Stochastic gradient descent)}.
Wyniki osiπgniÍte przy uøyciu poszczegÛlnych optymalizatorÛw zamieszczone zosta≥y w tabeli \ref{tbl:optimizers}.

\begin{table}[h!]
	\caption{PorÛwnanie skutecznoúci sieci przy uøyciu poszczegÛlnych optymalizatorÛw}
	\label{tbl:optimizers}
\begin{tabular}{|l|l|}
\hline
\textbf{Optymalizator} & \textbf{årednia skutecznoúÊ {[}\%{]}} \\ \hline
Adam                   & 78                                    \\ \hline
SGD                    & 79                                    \\ \hline
RMSProp                & 76,5                                      \\ \hline
\end{tabular}
\end{table}

Optymalizatorem, ktÛry osiπga≥ najlepsze wyniki by≥ SGD. Dodatkowo optymalizator tego typu bardzo dobrze sprawdza≥ siÍ rÛwnieø w parze z adaptacyjnie dobieranym wspÛ≥czynnikiem uczenia \ref{adaptive_learning_rate}.

\subsection{Adaptacyjny wspÛ≥czynnik uczenia}
\label{adaptive_learning_rate}
NastÍpnym krokiem jest dodanie mechanizmu, ktÛry pozwoli na adaptacyjny dobÛr wspÛ≥czynnika uczenia. SieÊ przeucza siÍ, gdyø w pÛüniejszych epokach uczy siÍ zbyt intensywnie istniejπcych juø reprezentacji danych. WspÛ≥czynnik uczenia kontroluje stopieÒ nauki sieci i moøe byÊ on ustawiany dynamicznie. Za pomocπ wczeúniej opisywanych \textit{callback}'Ûw dodany zosta≥ \textit{LearningRateScheduler}, ktÛry bÍdzie obniøa≥ wspÛ≥czynnik uczenia wraz z postÍpem epok uczπcych.
Listing \ref{lst:learning_rate_scheduler} przedstawia regu≥Í, wed≥ug ktÛrej bÍdzie on modyfikowany.

\begin{lstlisting}[caption=Regu≥a modyfikacji wspÛ≥czynnika uczenia, language=Python, label={lst:learning_rate_scheduler}, float=!h]
def step_decay(epoch):    
    initial_lrate=0.1
    drop=0.6
    epochs_drop = 10.0
    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
    
    return lrate
\end{lstlisting}

Dynamiczna zmiana wspÛ≥czynnika uczenia pozwoli≥a na czÍúciowe zredukowanie nadmiernego dopasowania sieci, jednak wciπø zaobserwowaÊ moøna jej przeuczanie. Mimo to skutecznoúÊ zosta≥a poprawiona do blisko \textbf{80,5\%} \ref{fig:adaptive_learning_rate}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial5/conv-adaptive-rate.png}
	\caption{Wyniki modelu po dodaniu zmiennego wspÛ≥czynnika uczenia}
	\label{fig:adaptive_learning_rate}
\end{figure}

\subsection{Modyfikacja rozmiaru sieci}
Ostatnim wykonanym krokiem by≥a modyfikacja rozmiaru sieci oraz poszczegÛlnych warstw. Wprowadzanych by≥o wiele zmian, na podstawie ktÛrych moøna by≥o zaobserwowaÊ, øe zbyt ma≥y rozmiar sieci zmniejsza≥ skutecznoúÊ modelu, gdyø nie mia≥ on wtedy wystarczajπcej przestrzeni do zapamiÍtania wszystkich waønych informacji. Rozbudowywanie modelu poczπtkowo zwiÍksza≥o skutecznoúÊ, jednak po osiπgniÍciu pewnego progu dodawanie kolejnych neuronÛw oraz warstw przynosi≥o jedynie negatywne skutki.

Listing \ref{lst:conv_final} zawiera model, ktÛry podczas testÛw osiπgnπ≥ najlepsze rezultaty. Zawiera on 3 warstwy konwolucyjne z kolejno 64, 64 i 32 filtrami o rozmiarze 3x3. Po kaødej z warstw zastosowano warstwÍ \textit{BatchNormalization}, funkcjÍ aktywacji \textit{relu} oraz warstwÍ \textit{max pooling}'u z filtrem o rozmiarze 2x2. W nastÍpstwie warstw konwolucyjnych sieÊ posiada 3 warstwy typu \textit{fully connected} rozdzielonych warstwami \textit{Dropout}. Warstwy posiadajπ rozmiar 64 oraz 32 neurony z warstwπ wyjúciowπ sk≥adajπcπ siÍ z jednego neuronu i sigmoidalnπ funkcjπ aktywacji \textit{(ang. sigmoid)}, ktÛra zwraca na wyjúciu wartoúÊ 0 lub 1. Model skompilowany zosta≥ przy uøyciu optymalizatora \textit{SGD} i jako funkcja strat uøyta zosta≥a \textit{binary crossentropy}. 

\begin{lstlisting}[caption=Ostateczny model sieci konwolucyjnej, language=Python, label={lst:conv_final}, float=h! ]
model = Sequential()
    
model.add(Conv2D(64,(3,3),strides = (1,1),name='layer_conv1',padding='same', input_shape=input_shape))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D((2,2),name='maxPool1'))

model.add(Conv2D(64,(3,3),strides = (1,1),name='layer_conv2',padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D((2,2),name='maxPool2'))

model.add(Conv2D(32,(3,3),strides = (1,1),name='conv3',padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D((2,2),name='maxPool3'))

model.add(Flatten())
model.add(Dense(64,activation = 'relu',name='fc0'))
model.add(Dropout(0.25))
model.add(Dense(32,activation = 'relu',name='fc1'))
model.add(Dropout(0.25))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=SGD(lr=0.01, momentum=0.5, decay=0.0, nesterov=False), 
              loss='binary_crossentropy',
              metrics=['acc'])
\end{lstlisting}

Podsumowanie modelu wygenerowane za pomocπ funkcji \textit{model.summary()} pokazujπce uøyte warstwy, rozmiary wyjúciowe oraz iloúÊ parametrÛw przedstawia siÍ nastÍpujπco \ref{lst:conv_summary}.

\begin{lstlisting}[caption=Podsumowanie modelu, language=Python, label={lst:conv_summary}]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
layer_conv1 (Conv2D)         (None, 200, 16, 64)       640       
_________________________________________________________________
batch_normalization_88 (Batc (None, 200, 16, 64)       256       
_________________________________________________________________
activation_88 (Activation)   (None, 200, 16, 64)       0         
_________________________________________________________________
maxPool1 (MaxPooling2D)      (None, 100, 8, 64)        0         
_________________________________________________________________
layer_conv2 (Conv2D)         (None, 100, 8, 64)        36928     
_________________________________________________________________
batch_normalization_89 (Batc (None, 100, 8, 64)        256       
_________________________________________________________________
activation_89 (Activation)   (None, 100, 8, 64)        0         
_________________________________________________________________
maxPool2 (MaxPooling2D)      (None, 50, 4, 64)         0         
_________________________________________________________________
conv3 (Conv2D)               (None, 50, 4, 32)         18464     
_________________________________________________________________
batch_normalization_90 (Batc (None, 50, 4, 32)         128       
_________________________________________________________________
activation_90 (Activation)   (None, 50, 4, 32)         0         
_________________________________________________________________
maxPool3 (MaxPooling2D)      (None, 25, 2, 32)         0         
_________________________________________________________________
flatten_30 (Flatten)         (None, 1600)              0         
_________________________________________________________________
fc0 (Dense)                  (None, 64)                102464    
_________________________________________________________________
dropout_59 (Dropout)         (None, 64)                0         
_________________________________________________________________
fc1 (Dense)                  (None, 32)                2080      
_________________________________________________________________
dropout_60 (Dropout)         (None, 32)                0         
_________________________________________________________________
dense_30 (Dense)             (None, 1)                 33        
=================================================================
Total params: 161,249
Trainable params: 160,929
Non-trainable params: 320
\end{lstlisting}

Na listingu \ref{lst:results} zamieszczone zosta≥y logi zawierajπce wyniki skutecznoúci sieci najlepszych modeli z poszczegÛlnych \textit{fold}'Ûw zebrane podczas trzech iteracji. årednia skutecznoúÊ modelu to oko≥o \textbf{82\%} (\textit{grand mean of average accuracy}). 

\begin{lstlisting}[caption=Wyniki skutecznoúci, language=Python, label={lst:results}]
Iteration 1

Fold:  0
--Best model validation accuracy: 79.65%

Fold:  1
--Best model validation accuracy: 83.93%

Fold:  2
--Best model validation accuracy: 83.04%

Fold:  3
--Best model validation accuracy: 84.82%

Fold:  4
--Best model validation accuracy: 86.61%

Fold:  5
--Best model validation accuracy: 77.68%

Fold:  6
--Best model validation accuracy: 81.08%

Fold:  7
--Best model validation accuracy: 78.38%

Fold:  8
--Best model validation accuracy: 81.98%

Fold:  9
--Best model validation accuracy: 83.78%


Best models average validation accuracy: 0.820943
Best models standard deviation of accuracy: 0.027431
Iteration 1 time: 511.17 seconds


Iteration 2

Fold:  0
--Best model validation accuracy: 78.76%

Fold:  1
--Best model validation accuracy: 83.04%

Fold:  2
--Best model validation accuracy: 83.93%

Fold:  3
--Best model validation accuracy: 84.82%

Fold:  4
--Best model validation accuracy: 87.50%

Fold:  5
--Best model validation accuracy: 75.89%

Fold:  6
--Best model validation accuracy: 77.48%

Fold:  7
--Best model validation accuracy: 83.78%

Fold:  8
--Best model validation accuracy: 78.38%

Fold:  9
--Best model validation accuracy: 84.68%


Best models average validation accuracy: 0.818264
Best models standard deviation of accuracy: 0.036667
Iteration 2 time: 527.52 seconds


Iteration 3

Fold:  0
--Best model validation accuracy: 81.42%

Fold:  1
--Best model validation accuracy: 85.71%

Fold:  2
--Best model validation accuracy: 83.93%

Fold:  3
--Best model validation accuracy: 84.82%

Fold:  4
--Best model validation accuracy: 86.61%

Fold:  5
--Best model validation accuracy: 76.79%

Fold:  6
--Best model validation accuracy: 77.48%

Fold:  7
--Best model validation accuracy: 81.98%

Fold:  8
--Best model validation accuracy: 82.88%

Fold:  9
--Best model validation accuracy: 84.68%


Best models average validation accuracy: 0.8263
Best models standard deviation of accuracy: 0.031417
Iteration 3 time: 643.31 seconds


~~~Grand mean of average accuracy: 0.821836
~~~Grand mean of standard deviation accuracy: 0.031839
\end{lstlisting}

\section{Weryfikacja i ocena otrzymanych rezultatÛw}
Przedstawione rezultaty by≥y dotychczas sprawdzane jedynie na danych walidacyjnych. Prawdziwym sprawdzianem modelu jest zweryfikowanie jego skutecznoúci na wczeúniej wydzielonym, odrÍbnym zbiorze testowym, ktÛry do tej pory nie bra≥ udzia≥u w procesie nauki. Model, ktÛry nie jest przeuczony i posiada dobrπ zdolnoúÊ generalizacji powinien osiπgnπÊ podobnπ skutecznoúÊ dla danych testowych.

W celu weryfikacji na danych testowych, do modelu sieci wczytane zosta≥y wagi zapisane na dysku, ktÛre najlepiej radzi≥y sobie podczas klasyfikacji na danych walidacyjnych. Proces sprawdzania skutecznoúci na danych testowych zosta≥ wywo≥any osobno dla kaødego modelu naczonego poszczegÛlnπ kombinacjπ danych. DziÍki temu z uzyskanych wynikÛw rÛwnieø moøna policzyÊ úredniπ, ktÛra bÍdzie lepiej oddawa≥a rzeczywisty stan, niø pojedynczy pomiar. 

Na modelu wywo≥ana zosta≥a metoda \textit{evaluate}, ktÛrej parametrami by≥y dane wejúciowe oraz wyjúciowe ze zbioru testowego. Wynikiem wykonania tych poleceÒ (listing \ref{lst:test_verification}) jest skutecznoúÊ modelu na danych, ktÛre zosta≥y uøyte po raz pierwszy (listing \ref{lst:test_results}).

\begin{lstlisting}[caption=Sprawdzanie skutecznoúci modelu na danych testowych, language=Python, label={lst:test_verification}]
model.load_weights("tmp/best_model.h5")
model.evaluate(x_test_3d, y_test, batch_size=16, verbose=0)
\end{lstlisting}

\begin{lstlisting}[caption=SkutecznoúÊ modelu na danych testowych, language=Python, label={lst:test_results}]
Iteration 1

Fold:  0
--Best model test accuracy: 77.97%

Fold:  1
--Best model test accuracy: 79.66%

Fold:  2
--Best model test accuracy: 79.66%

Fold:  3
--Best model test accuracy: 72.88%

Fold:  4
--Best model test accuracy: 79.66%

Fold:  5
--Best model test accuracy: 79.66%

Fold:  6
--Best model test accuracy: 74.58%

Fold:  7
--Best model test accuracy: 81.36%

Fold:  8
--Best model test accuracy: 76.27%

Fold:  9
--Best model test accuracy: 77.97%

Best models average test accuracy: 0.779661


Iteration 2

Fold:  0
--Best model test accuracy: 74.58%

Fold:  1
--Best model test accuracy: 79.66%

Fold:  2
--Best model test accuracy: 74.58%

Fold:  3
--Best model test accuracy: 74.58%

Fold:  4
--Best model test accuracy: 77.97%

Fold:  5
--Best model test accuracy: 72.88%

Fold:  6
--Best model test accuracy: 67.80%

Fold:  7
--Best model test accuracy: 71.19%

Fold:  8
--Best model test accuracy: 79.66%

Fold:  9
--Best model test accuracy: 74.58%

Best models average test accuracy: 0.747458


Iteration 3

Fold:  0
--Best model test accuracy: 79.66%

Fold:  1
--Best model test accuracy: 74.58%

Fold:  2
--Best model test accuracy: 76.27%

Fold:  3
--Best model test accuracy: 77.97%

Fold:  4
--Best model test accuracy: 79.66%

Fold:  5
--Best model test accuracy: 76.27%

Fold:  6
--Best model test accuracy: 72.88%

Fold:  7
--Best model test accuracy: 83.05%

Fold:  8
--Best model test accuracy: 77.97%

Fold:  9
--Best model test accuracy: 81.36%

Best models average test accuracy: 0.779661


~~~Grand mean of average test accuracy: 0.768927
\end{lstlisting}

OsiπgniÍte wyniki nie sπ identyczne jak te uzyskane na danych walidacyjnych, jednak sπ one wystarczajπco podobne, aby stwierdziÊ, øe model dobrze radzi sobie z nowymi danymi. Wynik na poziomie prawie \textbf{77\%} (76.89\%) na danych testowych jest zadowalajπcy i przekracza za≥oøony prÛg 70\% ustalony w rozdziale \ref{expectation}.
