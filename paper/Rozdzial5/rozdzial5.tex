\chapter{Rozwi¹zanie praktycznego problemu}
\label{chapter_5}
Proces uczenia sieci neuronowej stworzony zosta³ w œrodowisku Jupyter Notebook. Umo¿liwia ono tworzenie dokumentów zawieraj¹cych opisy, wykresy oraz wykonywalne kody Ÿród³owe. Nazwa Jupyter wziê³a siê z tego, ¿e œrodowisko umo¿liwia pracê w trzech jêzykach: Julia, Python oraz R.

Dokument sk³ada siê z wielu komórek, które mog¹ byæ wywo³ywane niezale¿nie.
Przewag¹ kodu pisanego w tym œrodowisku nad tradycyjnymi skryptami jest oferowana przez nie mo¿liwoœæ zapamiêtywania stanu zmiennych. Dziêki temu nie ma potrzeby ka¿dorazowego wywo³ywania ca³ego kodu, a jedynie fragmentów, które w danej chwili s¹ potrzebne np. jednorazowe przygotowanie danych, a nastêpnie wykonywanie jedynie procesu uczenia.

Proces uczenia sk³ada siê z kilku kroków:
\begin{itemize}
	\item pobranie i przygotowanie danych za pomoc¹ przygotowanych skryptów \ref{data_preparation},
	\item podzia³ danych na zbiory s³u¿¹ce do nauki oraz testowania przy pomocy k-krotnej 
walidacji krzy¿owej \textit{(ang. k-fold cross-validation)} \ref{validation_method},
	\item utworzenie modelu sieci neuronowej \ref{build_model},
	\item rozpoczêcie uczenia modelu \ref{run_learning},
	\item sprawdzenie skutecznoœci i przedstawienie wyników \ref{verify_accuracy}.
\end{itemize}

Wszystkie wymienione kroki przedstawiaj¹ pe³en proces uczenia, który pokazuje z jak¹ dok³adnoœci¹ model jest w stanie wykrywaæ stany padaczkowe.

\section{Przygotowanie danych}
\label{data_preparation}
W celu wykorzystania dostêpnych danych w procesie uczenia sieci neuronowej nale¿y je najpierw odpowiednio przygotowaæ.
Dostarczone zosta³y dane w postaci plików tekstowych z liczbami reprezentuj¹cymi wartoœci zmierzone przy pomocy elektroencefalogramu. W poszczególnych kolumnach znajduj¹ siê wartoœci odpowiadaj¹ce konkretnym kana³om (\textit{EEG\_FP1\_F3}, \textit{EEG\_FP2\_F4} itd.). Ostatnia kolumna (o nag³ówku \textit{t}) przedstawia czas w sekundach, w którym mia³ miejsce pomiar. 

Dostêpne s¹ dane 104 pacjentów, u których wyst¹pi³y ataki. Ka¿dy z nich posiada odczyty wykonane z czêstotliwoœci¹ 500Hz przez blisko 15 minut. Jest to oko³o 450 000 linii w ka¿dym pliku. Dane zajmuj¹ 6,5 GB.
Fragment jednego z plików przedstawiony zosta³ na listingu \ref{lst:eeg}:
\begin{landscape}
\begin{lstlisting}[caption=Dane liczbowe z odczytu EEG, label=lst:eeg]
"EEG_FP1_F3" "EEG_FP2_F4" "EEG_F3_C3" "EEG_F4_C4" "EEG_C3_P3" "EEG_C4_P4" "EEG_P3_O1" "EEG_P4_O2" "EEG_FP1_F7" "EEG_FP2_F8" "EEG_F7_T3" "EEG_F8_T4" "EEG_T3_T5" "EEG_T4_T6" "EEG_T5_O1" "EEG_T6_O2" "t"
-1.7032  -3.3727 5.9014 2.512 2.6404 4.0193 16.9162 35.4401 5.3506 1.1355 -1.0843 -3.9932 9.4594 14.8409 10.0322 26.6143 0
-3.1702 -4.5883 5.8762 2.9638 3.5952 3.9019 17.1137 36.3634 4.3875 0.543 -1.356 -4.5697 10.309 14.1082 10.0724 28.5593 0.002
-4.2669 -5.2157 5.6852 3.1698 4.0053 3.6091 16.4135 36.7024 2.9881 0.4522 -1.7584 -5.4206 11.1134 13.3778 9.4926 29.8509 0.004
-4.8106 -5.2287 5.2889 2.6745 3.714 3.0257 14.9097 36.4086 1.6189 0.2802 -2.1206 -6.4522 11.2876 12.7727 8.3198 30.2831 0.006
-5.0684 -5.1176 4.7665 1.7354 2.9116 2.354 12.7327 35.6144 0.5718 -0.618 -2.57 -7.3378 10.6144 12.2487 6.7256 30.2934 0.008
-5.3637 -5.1568 4.399 1.03 1.764 1.9509 10.2011 34.6652 -0.2974 -2.1804 -3.1603 -7.7025 9.3368 11.727 5.1179 30.6433 0.01
-5.6214 -5.0654 4.3918 0.9508 0.24 1.8783 7.7188 33.8484 -1.2102 -3.7284 -3.7707 -7.4767 7.7603 11.2122 3.9518 31.6004 0.012
-5.3918 -4.3726 4.6296 1.2678 -1.7391 1.9194 5.4296 32.8928 -2.0525 -4.7365 -4.371 -6.9662 6.0031 10.7833 3.3565 32.6295 0.014
-4.4216 -2.9806 4.8926 1.4897 -4.0924 1.8759 3.1044 31.2302 -2.5055 -5.2621 -5.1323 -6.5737 4.1319 10.4842 2.993 32.964 0.016
-3.0671 -1.3206 5.1772 1.4303 -6.5959 1.8178 0.4606 28.7023 -2.4988 -5.6443 -6.219 -6.4834 2.3209 10.2523 2.3732 32.5009 0.018
-2.0125 0.0126 5.624 1.3708 -9.0008 1.8601 -2.4571 25.9613 -2.4216 -6.0744 -7.4599 -6.5563 0.7207 10.0158 1.3118 31.8217 0.02
-1.6282 0.6792 6.1788 1.668 -11.1278 1.9872 -5.285 23.879 -2.7505 -6.5044 -8.4291 -6.466 -0.7268 9.7492 0.0498 31.4306 0.022
-1.6657 0.8034 6.6003 2.2822 -12.8962 2.0405 -7.673 22.7264 -3.4955 -6.877 -8.9121 -5.952 -2.1528 9.4663 -1.074 31.2145 0.024
-1.5719 0.7576 6.7409 2.7736 -14.344 1.9775 -9.5717 21.9936 -4.0627 -7.2927 -9.1401 -5.0143 -3.5228 9.1023 -2.0194 30.7051 0.026
-1.0657 0.7511 6.7409 2.833 -15.5319 1.8759 -11.2415 20.9346 -3.915 -7.9234 -9.4655 -3.9029 -4.6154 8.5574 -3.1008 29.6605 0.028
-0.4658 0.64 6.7481 2.6388 -16.4105 1.8674 -12.9338 19.1106 -3.3982 -8.8312 -9.7774 -2.8922 -5.2413 7.7923 -4.6393 28.1837 0.03
-0.4283 0.1433 6.6724 2.5675 -16.8789 1.9739 -14.6844 16.5569 -3.66 -9.9301 -9.5996 -2.0969 -5.4112 6.8556 -6.6483 26.4136 0.032
-1.2579 -0.837 6.2977 2.6904 -17.0425 2.1591 -16.2868 13.5512 -5.3513 -11.1628 -8.8048 -1.3918 -5.2607 5.7612 -8.8668 24.3656 0.034
-2.5281 -2.1571 5.5879 2.726 -17.2241 2.3092 -17.4808 10.3421 -7.6669 -12.6201 -7.9998 -0.5653 -4.9359 4.4372 -11.0386 21.9677 0.036
-3.4233 -3.5492 4.6692 2.5041 -17.6387 2.3055 -18.1227 6.9393 -9.0697 -14.3688 -7.8724 0.5427 -4.5573 2.8026 -13.0119 19.2251 0.038
-3.5686 -4.719 3.6748 2.1871 -18.0668 2.0102 -18.226 3.2427 -9.0093 -16.2083 -8.3252 1.8902 -4.2347 0.9176 -14.6128 16.1274 0.04
-3.292 -5.4248 2.7057 1.9731 -18.0892 1.3869 -17.9791 -0.8025 -8.4321 -17.699 -8.5968 3.1857 -3.9831 -0.9698 -15.6362 12.613 0.042
-3.1795 -5.5751 2.0283 1.7869 -17.5715 0.4949 -17.5975 -5.0931 -8.516 -18.5016 -8.1441 4.0158 -3.6433 -2.5673 -16.0108 8.6663 0.044
\end{lstlisting}
\end{landscape}

Dodatkowo dostarczony zosta³ plik przechowuj¹cy wyznaczone przez lekarza momenty wyst¹pienia ataków dla ka¿dego z pacjentów. Plik w kolejnych wierszach zawiera punkty w czasie rozdzielone przecinkami (w sekundach).
Ka¿dy wiersz odpowiada jednemu pacjentowi, dlatego plik zawiera 104 wiersze. Fragment pliku z czasami wyst¹pienia ataków:
\begin{lstlisting}[caption=Punkty w czasie wyst¹pienia ataków (w sekundach)] 
835,853,865,873,889,908
18,48,110,309,466,618,757
216,239
44,329,501,559,622
36,190,406,576,714,754
158,510,917
622,653,676,737
\end{lstlisting}

Dane z kilku kana³ów z naniesionym momentem wyst¹pienia ataku przedstawione zosta³y na rysunku \ref{fig:data_vis}.
Mo¿na zaobserowaæ, ¿e bez posiadania specjalistycznej wiedzy ciê¿ko by³oby okreœliæ moment wyst¹pienia ataku. 
Fragmenty odczytu w momencie ataku na pierwszy rzut oka nieznacznie ró¿ni¹ siê od pozosta³ych.
Œwiadczy o wiele wiêkszej z³o¿onoœci problemu w porównaniu do zadañ, z którymi mózg praktycznie ka¿dego cz³owieka radzi sobie doskonale np. rozpoznawanie liczb, odró¿nianie psów od kotów itp.

\begin{landscape}
\begin{figure}[h!]
	\centering
	\includegraphics[width=25cm]{Rysunki/Rozdzial2/eeg_vis.png}
	\caption{Wykres przedstawiaj¹cy odczyt EEG z zaznaczonym momentem wyst¹pienia ataku}
	\label{fig:data_vis}
\end{figure}
\end{landscape}

W celu wykorzystania przedstawionych danych w procesie uczenia sieci neuronowej musz¹ zostaæ one odpowiednio przygotowane.
Wybrane podejœcie zak³ada podzielenie danych na okna czasowe o okreœlonej d³ugoœci. Znane s¹ jedynie pocz¹tki ataków, jednak nie wiadomo jak d³ugo trwa³y. D³ugoœæ okna czasowego, wed³ug którego podzielone zostan¹ dane bêdzie wiêc jednym z parametrów, który nale¿y dobraæ w celu uzyskania najlepszych rezultatów.

Do przygotowania danych stworzony zosta³ skrypt, który wykonuje nastêpuj¹ce czynnoœci:
\begin{itemize}
	\item wczytanie danych pacjentów oraz informacji o atakach,
	\item przetworzenie danych do czêstotliwoœci 100Hz,
	\item podzia³ danych na okna czasowe o d³ugoœci przekazanej jako parametr (w sekundach),
	\item normalizacja danych - œrednia 0, odchylenie standardowe 1,
	\item konwersja danych dotycz¹cych czasu wyst¹pienia ataków na postaæ tzw. "jeden z n" \textit{(ang. one-hot encoding)},
	\item wyœwietlanie informacji o postêpie przetwarzanych plików,
	\item zapis pobranych danych do plików tymczasowych umo¿liwiaj¹cych szybszy odczyt.
\end{itemize}

Technicznie skrypty zosta³y podzielone na 2 pliki:
\begin{itemize}
\item data\_reader.py,
\item chunks\_creator.py.
\end{itemize}

Pierwszy z nich zajmuje siê wczytywaniem i korzysta z drugiego w celu stworzenia okien czasowych.
G³ówn¹ funkcj¹ dostarczan¹ przez skrypt, która umo¿liwia wykonanie wszystkich wymienionych wy¿ej czynnoœci jest \textit{get\_data()}. Jako parametr przyjmuje ona czas w sekundach, który oznacza d³ugoœæ okna czasowego. Znajduje siê ona na koñcu pliku, gdy¿ z uwagi na specyfikê jêzyka wszystkie wykorzystywane przez ni¹ funkcje musz¹ byæ wczeœniej zadeklarowane. 
Kod skryptu pobieraj¹cego dane rozszerzony o komentarze opisuj¹ce poszczególne kroki zaprezentowany zosta³ na listingu \ref{lst:data-reader.py}.

\begin{lstlisting}[caption=data\_reader.py, language=Python, label={lst:data-reader.py}]
import os
import numpy as np
import pickle

from chunks_creator import prepare_chunks
from chunks_creator import flatten_chunks

from sklearn.preprocessing import StandardScaler

INPUT_DATA_FILE_PATH='tmp/input-{}sec.pckl'

DATA_FREQUENCY = 500
SAMPLING_RATE = 5
FREQUENCY_TO_SAMPLING_RATIO = DATA_FREQUENCY // SAMPLING_RATE


# Konwertuje plik z danymi pacjenta z postaci tekstowej do tablicowej oraz zmienia czêstotliwoœæ próbkowania do 100Hz
def parse_file(file, sampling_rate):
    lines = file.split('\n')
    headers = lines[0].split('\t')
    # to one before last because the last one is empty
    data = lines[1:-1]

    number_of_lines = len(data)

    float_data = np.zeros((number_of_lines, len(headers)))
    for line_number, line in enumerate(data):
        values = [float(value) for value in line.split('\t')]
        float_data[line_number, :] = values

    return float_data[::sampling_rate], headers


# Wczytuje dane pacjentów z dysku
def read_input_files(end, data_path, sampling_rate):
    input_path = os.path.join(data_path, 'input_500Hz/sick')
    input_file_names = os.listdir(input_path)
    input_file_names.sort(key=int)

    start = None

    files_content = []
    for file_name in input_file_names[start:end]:
        file_path = os.path.join(input_path, file_name)
        file = open(file_path, 'r')
        (columns, headers) = parse_file(file.read(), sampling_rate)
        print('Loaded input file:', file_name)
        file.close()
        files_content.append(columns)
    print('--Input files loaded--')
    return files_content, headers


def create_target_index(value, frequency_to_sampling_ratio):
    value = int(value)
    return int(value * frequency_to_sampling_ratio)


# Odczytuje informacje dotycz¹ce czasów ataków i konwertuje do postaci one-hot
def read_target_files(end, data_path, sampling_rate, data_frequency):
    frequency_to_sampling_ratio = data_frequency // sampling_rate
    targets_path = os.path.join(data_path, 'targets')
    targets_file_name = os.listdir(targets_path)[0]
    targets_file_path = os.path.join(targets_path, targets_file_name)

    file = open(targets_file_path, 'r')
    targets_content = file.read()
    file.close()

    lines = targets_content.split('\n')[:-1]
    targets = []
    for number, line in enumerate(lines, 1):
        targets.append([(int(value), create_target_index(value, frequency_to_sampling_ratio)) for value in line.split(',')])
    print('--Target files loaded--')
    return targets[:end]


# Pobiera dane pacjentów oraz czasy ataków
def read_data(data_path, sampling_rate, data_frequency, end=104):
    (input_data, headers) = read_input_files(end, data_path, sampling_rate)
    targets_data = read_target_files(end, data_path, sampling_rate, data_frequency)

    return input_data, targets_data, headers


# Odczytuje dane i zapisuje zmienne do pliku tymczasowego
def load_data_to_file(chunk_size_in_seconds):
    (input_data, target, headers) = read_data(data_path='data', 
                                              sampling_rate=SAMPLING_RATE, 
                                              data_frequency=DATA_FREQUENCY)

    with open(INPUT_DATA_FILE_PATH.format(chunk_size_in_seconds), 'wb') as input_variable_file:
        pickle.dump([input_data, target, headers], input_variable_file)

    del input_data, target, headers
    

# Normalizuje dane u¿ywaj¹c obiektu StandardScaler(). Zwrócone dane posiadaj¹ œredni¹ 0 oraz odchylenie standardowe 1.    
def normalize(x, y):
    scalers = {}
    for channel_number in range(x.shape[1]):
        scalers[channel_number] = StandardScaler()
        x[:, channel_number, :] = scalers[channel_number].fit_transform(x[:, channel_number, :]) 
    return x, y.astype(int)


# Pobiera zmienne z danymi z utworzonego pliku tymczasowego
def load_input_data(chunk_size_in_seconds):
    with open(INPUT_DATA_FILE_PATH.format(chunk_size_in_seconds), 'rb') as input_data_file:
        input_data, target, headers = pickle.load(input_data_file)
    
    return input_data, target, headers


# Pobiera dane wykorzystuj¹c funkcjê load_input_data() i wywo³uje funkcjê prepare_chunks() z pliku chunks_creator.py w celu utworzenia okien czasowych z danymi. Utworzone porcje danych s¹ nastêpnie poddawane normalizacji.
def prepare_data(chunk_size_in_seconds):
    input_data, target, headers = load_input_data(chunk_size_in_seconds)
    
    chunks_input, chunks_target = prepare_chunks(input_data, 
                                                target, 
                                                chunk_size_in_seconds=chunk_size_in_seconds, 
                                                ratio=FREQUENCY_TO_SAMPLING_RATIO)
    x, y = flatten_chunks(chunks_input, chunks_target)
    x, y = normalize(x, y)
    
    return x, y


# G³ówna funkcja skryptu, która zwraca odpowiednio przygotowane dane. Przy pierwszym uruchomieniu wczytuje dane z dysku do zmiennych i zapisuje je do pliku tymczasowego. Odczyt zmiennych z danymi z pliku binarnego umo¿liwia szybszy dostêp przy kolejnych uruchomieniach, gdy¿ otwieranie du¿ych plików tekstowych jest czasoch³onne.
def get_data(chunk_size_in_seconds):
    file_exists = os.path.isfile(INPUT_DATA_FILE_PATH.format(chunk_size_in_seconds))
    
    if not file_exists:
        load_data_to_file(chunk_size_in_seconds)
        
    return prepare_data(chunk_size_in_seconds)
\end{lstlisting}

Funkcja \textit{prepare\_data()} w powy¿szym skrypcie korzysta z osobnego pliku o nazwie \textit{chunks\_creator.py}, w którym zosta³y zdefiniowane funkcje tworz¹ce okna czasowe z danymi.
Dla danych ka¿dego z pacjentów tworzonych jest \textit{2 * n} okien czasowych, gdzie \textit{n} oznacza iloœæ zdiagnozowanych ataków. 

Porcje danych zawieracj¹ce ataki tworzone s¹ na pocz¹tku ka¿dego z nich i trwaj¹ przez okreœlony czas podany w parametrze. Nastêpnie tworzone jest \textit{n} kolejnych porcji danych zawieraj¹cych dane liczbowe z okresu, w którym nie wyst¹pi³ atak.
Utworzone okna czasowe zawieraj¹ wiêc tak¹ sam¹ iloœæ danych z atakami oraz bez.
Dodatkowo tworzona jest tablica zawieraj¹ca informacje o tym czy dla danego okna czasowego wyst¹pi³  \textit{(wartoœæ 1)} lub nie wyst¹pi³ \textit{(wartoœæ 0)} atak.

Implementacja skryptu odopowiadaj¹cego za przetwarzanie danych do postaci okien czasowych przedstawiona zosta³a na listlingu \ref{lst:chunks-creator.py}.

\begin{lstlisting}[caption=chunks\_creator.py, language=Python, label={lst:chunks-creator.py}]
import random
import numpy as np


# Tworzy porcje danych, w których wyst¹pi³y ataki.
def create_chunks_with_seizures(patient_data, seizure_seconds, chunk_size):
    number_of_chunks = len(seizure_seconds)

    chunks_input = np.zeros((number_of_chunks, chunk_size, 17))
    chunks_target = np.zeros(number_of_chunks)

    for seizure_number in range(0, number_of_chunks):
        (seizure_time, seizure_index) = seizure_seconds[seizure_number]
        chunk_start_index = seizure_index
        chunk_end_index = chunk_start_index + chunk_size
        chunks_input[seizure_number] = patient_data[chunk_start_index:chunk_end_index, :]
        # atak oznaczony wartoœci¹ '1'
        chunks_target[seizure_number] = 1

    return (chunks_input, chunks_target)


# Sprawdza czy podany fragment znajduje siê w zasiêgu ataku.
def is_in_seizure_range(index, seizure_seconds, chunk_size):
    for (seizure_time, seizure_index) in seizure_seconds:
        seizure_start_index = seizure_index
        seizure_end_index = seizure_start_index + chunk_size
        if index in range(seizure_start_index, seizure_end_index):
            return True

    return False


# Tworzy pocz¹tek pojedynczej porcji danych, który wybierany jest losowo, jednak sprawdzane jest, aby nie zawiera³a ona momentów, w których wyst¹pi³ atak.
def create_non_seizure_data_start_index(data_size, chunk_size, seizure_seconds):
    start_index = random.randint(0, data_size - chunk_size)

    while (is_in_seizure_range(start_index, seizure_seconds, chunk_size)):
        start_index = random.randint(0, data_size - chunk_size)

    return start_index


# Tworzy porcje danych, w których nie wyst¹pi³y ataki.
def create_chunks_without_seizures(patient_data, seizure_seconds, chunk_size):
    number_of_chunks = len(seizure_seconds)

    chunks_input = np.zeros((number_of_chunks, chunk_size, 17))
    chunks_target = np.zeros(number_of_chunks)
    (data_size, channels) = patient_data.shape

    for chunk_number in range(0, number_of_chunks):
        chunk_start_index = create_non_seizure_data_start_index(data_size, chunk_size, seizure_seconds)

        chunk_end_index = chunk_start_index + chunk_size
        chunks_input[chunk_number] = patient_data[chunk_start_index:chunk_end_index, :]
        # brak ataku oznaczone wartoœci¹ '0'
        chunks_target[chunk_number] = 0

    return (chunks_input, chunks_target)


# Przystosowuje iloœæ wymiarów danych do procesu uczenia sieci neuronowej.
def flatten_chunks(chunks_input, chunks_target):
    train_input = []
    train_target = []

    for patient_number in range(0, len(chunks_input)):
        patient_data = chunks_input[patient_number]
        patient_targets = chunks_target[patient_number]
        for chunk_number in range(0, len(patient_data)):
            train_input.append(patient_data[chunk_number])
            train_target.append(patient_targets[chunk_number])

    train_input = np.array(train_input)
    train_target = np.array(train_target)

    train_input = train_input[:, :, :-1]
    
    return train_input, train_target 


# G³ówna funkcja skryptu zwracaj¹ca okna czasowe stworzone z danych pacjentów w postaci wielowymiarowej tablicy oraz tablicê zawieraj¹c¹ informacje o wyst¹pieniu ataków.
def prepare_chunks(input, target, chunk_size_in_seconds, ratio):
    chunk_size = chunk_size_in_seconds * ratio
    chunks_input = []
    chunks_target = []

    for patient_number in range(0, len(input)):
        patient_chunks_input = []
        patient_chunks_target = []
        seizure_seconds = target[patient_number]
        patient_data = input[patient_number]
        (seizure_chunks_input, seizure_chunks_target) = create_chunks_with_seizures(patient_data,seizure_seconds, chunk_size)
        patient_chunks_input.extend(seizure_chunks_input)
        patient_chunks_target.extend(seizure_chunks_target)

        (non_seizure_chunks_input, non_seizure_chunks_target) = create_chunks_without_seizures(patient_data, seizure_seconds, chunk_size)
        patient_chunks_input.extend(non_seizure_chunks_input)
        patient_chunks_target.extend(non_seizure_chunks_target)

        chunks_input.append(np.array(patient_chunks_input))
        chunks_target.append(np.array(patient_chunks_target))

    return np.array(chunks_input), np.array(chunks_target)

\end{lstlisting}

Przedstawione skrypty umo¿liwiaj¹ pobranie odpowiednio przygotowanych danych, które nastêpnie mog¹ byæ u¿yte w procesie uczenia sieci neuronowej.

\section{Metoda oceny wyników}
\label{validation_method}
Do zweryfikowania skutecznoœci modelu wymagane jest podzielenie danych na odpowiednie zbiory. Korzystanie z jednego, identycznego zbioru danych do nauki oraz testowania jest b³êdem, gdy¿ mo¿e wprowadziæ z³udne wra¿enie, ¿e model posiada bardzo dobr¹ skutecznoœæ. Sieæ neuronowa, której dzia³anie weryfikowane jest na danych, które pos³u¿y³y do nauki bêdzie posiadaæ skutecznoœæ blisk¹ 100\%. Dzieje siê tak dlatego, ¿e umie ona rozpoznawaæ te konkretne dane, gdy¿ zna dla nich wartoœci wyjœciowe, które podane zosta³y podczas nauki. W ten sposób nauczony model bêdzie natomiast posiada³ bardzo nisk¹ skutecznoœæ, gdy na jego wejœcie podane zostan¹ dane, które nigdy wczeœniej nie zosta³y mu przedstawione.
W ka¿dym rodzaju uczenia maszynowego d¹¿y siê do uzyskania jak najwiêkszej zdolnoœci generalizacji, czyli skutecznoœci klasyfikacji danych, które nie zosta³y nigdy wczeœniej przekazane do modelu. Przeciwieñstwem tego jest tzw. \textit{nadmierne dopasowanie} (ang. \textit{overfitting}), czyli sytuacja, w której model zbytnio dopasowuje siê do danych ucz¹cych, co powoduje bardzo s³ab¹ zdolnoœæ generalizacji. Zjawisko to jest g³ównym problemem podczas uczenia modelu.

\subsection{Hold-out}
Podstawowym sposobem oceny wyników jest podzielenie danych na dwa zbiory: treningowy oraz testowy \textit{(ang. hold-out)}. Mo¿na dzieliæ je w ró¿nych proporcjach np. 80/20, 90/10, 95/5 itp. w zale¿noœci od iloœci posiadanych danych. Przy niewielkich zbiorach wydzielenie zbyt du¿ej liczby danych do zbioru testowego mo¿e powodowaæ niedobory w procesie nauki. Wydzielenie jedynie dwóch zbiorów danych nie rozwi¹zuje jednak do koñca problemu \textit{overfitting}'u, gdy¿ zbyt czêsta zmiana modelu i sprawdzanie jego skutecznoœci na danych testowych mo¿e spowodowaæ zbyt du¿e dopasowanie do zbioru testowego \cite{Chollet:2018}. Z tego wzglêdu czêsto stosowany jest dodatkowy zbiór walidacyjny \textit{(ang. validation set)}. W tym podejœciu model uczony jest na danych treningowych, sprawdzany na danych walidacyjnych i na podstawie tych wyników wprowadza siê modyfikacje w modelu. Zbiór testowy s³u¿y jedynie do ostatecznego sprawdzenia skutecznoœci modelu, gdy¿ s¹ to dane, które nigdy wczeœniej nie zosta³y u¿yte w procesie nauki. Dane dzielone s¹ w proporcjach np. 80/10/10, 90/5/5. Schemat tego podejœcia przedstawiony jest na rysunku \ref{fig:hold-out-validation}

\begin{figure}[h!]
	\centering
	\includegraphics[width=9cm]{Rysunki/Rozdzial5/hold-out-validation.pdf}
	\caption{Podzia³ danych na zbiory: treningowy, walidacyjny i testowy}
	\label{fig:hold-out-validation}
\end{figure}

Metoda ta rozwi¹zuje problem zbytniego dopasowywania siê do zbioru testowego, jednak równie¿ posiada wadê poprzedniego rozwi¹zania. Przy niewielkiej iloœci danych wydzielanie osobnych zbiorów mo¿e spowodowaæ, ¿e zbiór ucz¹cy bêdzie zbyt ma³y. W celu unikniêcia tego problemu mo¿na zastosowaæ tzw. walidacjê krzy¿ow¹ i jej iteracyjne rozszerzenie.


\subsection{Walidacja krzy¿owa}
Kolejn¹ metod¹ walidacji jest tzw. \textit{k}-krotna walidacja krzy¿owa \textit{(ang. k-fold cross-validation)}. Tak samo jak w przypadku metody \textit{holdout} nale¿y najpierw wydzieliæ zbiór testowy, który nie bêdzie bra³ udzia³u w procesie uczenia i walidacji. Pozosta³e dane dzielone s¹ na \textit{k} zbiorów, przewa¿nie jest to \textit{k = 5} lub \textit{k = 10}. Nastêpnie \textit{k}-krotnie powtarzany jest proces wyboru jednego \textit{i}-tego zbioru, gdzie \textit{i} oznacza kolejne liczby z przedzia³u \textit{<1; k>}. Zbiór o numerze \textit{i} wybierany jest jako zbiór walidacyjny, natomiast reszta \textit{k - 1} zbiorów s³u¿¹ jako dane ucz¹ce. Nale¿y pamiêtaæ, ¿eby w ka¿dym przebiegu uczyæ now¹ instancjê modelu. W rezultacie model uczony jest \textit{k}-krotnie na ró¿nych kombinacjach danych. Jako wynik koñcowy brana jest pod uwagê œrednia skutecznoœæ wszystkich modeli. Sposób podzia³u danych w tej metodzie ilustruje rysunek \ref{fig:cross-validation}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/cross-validation.pdf}
	\caption{Podzia³ danych wed³ug metody k-krotnej walidacji krzy¿owe}
	\label{fig:cross-validation}
\end{figure}

Przedstawion¹ metodê mo¿na rozszerzyæ do jej iteracyjnej wersji polegaj¹cej na tym, ¿e ca³y proces \textit{k}-krotnej walidacji krzy¿owej wykonywany jest \textit{n} razy. Dodatkowo po ka¿dej iteracji mo¿na zastosowaæ mieszanie danych. Walidacja t¹ metod¹ co prawda trwa o wiele d³u¿ej, gdy¿ model walidowany jest \textit{k * n} razy, jednak pozwala uzyskaæ bardziej wiarygodne wyniki, gdy nie jest dostêpna zbyt du¿a liczba danych.

\subsection{Implementacja walidacji}
\label{crossvalidation_implementation}
Zaimplementowane zosta³y obie metody podzia³u danych, jednak ze wzglêdu na swoje zalety metody iteracyjnej \textit{k}-krotnej walidacji krzy¿owej to w³asnie ona zosta³a u¿yta do walidacji w procesie uczenie. Podzia³ danych wed³ug obu metod zosta³ wykonany za pomoc¹ biblioteki \textit{sklearn}, która dostarcza przygotowane do tego celu funkcje (patrz listing \ref{lst:split}).
\begin{lstlisting}[caption=Podzia³ danych na podzbiory treningowe i testowe, language=Python, label={lst:split}, float=h!]
from sklearn.model_selection import train_test_split, StratifiedKFold
from data_reader import get_data

def load_data_kfold(folds_number, test_size=0.05)):
    x, y = get_data(CHUNK_SIZE_IN_SECONDS)
    
    x_train, x_test, y_train, y_test = train_test_split(x, 
                                                        y, 
                                                        test_size=test_size)
    
    folds = list(StratifiedKFold(n_splits=folds_number, 
                                 shuffle=True, 
                                 random_state=1).split(x_train, y_train))
    
    return folds, x_train, y_train, x_test, y_test

def load_data_train_test(test_size=0.05):
    x, y = get_data(CHUNK_SIZE_IN_SECONDS)
    
    x_train, x_test, y_train, y_test = train_test_split(x, 
                                                        y, 
                                                        test_size=test_size)
    
    return x_train, y_train, x_test, y_test
\end{lstlisting}

Tak przygotowane dane podawane s¹ do modelu w \textit{n} iteracjach za pomoc¹ prostej pêtli. Wyniki z ka¿dej iteracji zapisywane s¹ do tablicy, a na koniec obliczana jest ich œrednia. Proces iteracyjnego uruchamiania uczenia wraz z obliczaniem wyników zaprezentowany zosta³ na listingu \ref{lst:iteration_learning}

\begin{lstlisting}[caption=Iterecyjne wywo³anie procesu uczenia, language=Python, label={lst:iteration_learning}]
number_of_iterations = 5

avg_accuracies = []
std_accuracies = []

for iteration in range(0, number_of_iterations):
    iteration_number = iteration + 1 
    print("Iteration", iteration_number)
    
    score, best_model_score = run_pipeline(create_model=model,
                                           folds=folds,
                                           x=x_train,
                                           y=y_train,
                                           epochs=100)

    accuracy = [row[1] for row in best_model_score]

    avg_accuracy = np.mean(accuracy)
    print("Best models average validation accuracy: {}".format(round(avg_accuracy, 6)))
    
    avg_accuracies.append(avg_accuracy)
   
grand_mean_avg = np.mean(avg_accuracies)
print("~~~Grand mean of average accuracy: {}".format(round(grand_mean_avg, 6)))
\end{lstlisting}

Do uruchamiania procesu uczenia u¿ywana jest funkcja \textit{run\_pipeline}, która szczegó³owo opisana zostanie w rozdziale \ref{run_learning}.

\section{Uruchomienie uczenia w³aœciwego}
\label{run_learning}
Do uruchomienia procesu uczenia w³aœciwego s³u¿y funkcja \textit{run\_pipeline()}, która wykonuje nastêpuj¹ce czynnoœci:
\begin{itemize}
\item tworzy model na podstawie dostarczonych specyfikacji (\ref{build_model}),
\item pobiera odpowiednie porcje danych dla ka¿dego kroku z przytogowanych wczeœniej zbiorów (\ref{crossvalidation_implementation}),
\item tworzy listê tzw. \textit{callback}'ów (\ref{callbacks}),
\item uruchamia na modelu metodê rozpoczynaj¹c¹ uczenie (\ref{fit}),
\item wczytuje wagi najlepszego modelu i sprawdza jego skutecznoœæ (\ref{verify_accuracy}).
\end{itemize}

Kod metody \textit{run\_pipeline} zaprezentowany zosta³ na listingu \ref{lst:run-pipeline}. Poszczególne kroki zostan¹ opisane w kolejnych rozdzia³ach.

\begin{lstlisting}[caption=Implementacja metody uruchamiaj¹cej uczenie w³aœciwe, language=Python, label={lst:run-pipeline}]
def run_pipeline(create_model, folds, x, y, epochs):
    best_model_score = []
    
    for fold_number, (train_idx, val_idx) in enumerate(folds):
        print('\nFold: ', fold_number)
        # wybór odpowiednich podzbiorów
        x_train_cv = x[train_idx]
        y_train_cv = y[train_idx]
        x_valid_cv = x[val_idx]
        y_valid_cv = y[val_idx]
                
        input_shape = x.shape[1:]

        # stworzenie modelu
        model, model_description = create_model(input_shape)

        # utworzenie callback'ów
        callbacks = callbacks_list("{}. Fold: {}.".format(model_description, 
                                                          fold_number))
        # rozpoczêcie uczenia
        history = model.fit(x_train_cv,
                            y_train_cv,
                            epochs=epochs,
                            batch_size=16,
                            callbacks=callbacks,
                            validation_data=(x_valid_cv, y_valid_cv),
                            verbose=0)

        # wczytanie wag najlepszego modelu i sprawdzenie jego skutecznoœci
        model.load_weights("tmp/best_model.h5")
        best_model_score.append(model.evaluate(x_valid_cv, y_valid_cv, batch_size=16, verbose=0))
        print("--Best model validation accuracy: %.2f%%" % (best_model_score[fold_number][1]*100))
        
    return best_model_score
\end{lstlisting}

\subsection{Budowa modelu}
\label{build_model}
Model tworzony jest na podstawie dostarczonych specyfikacji w postaci funkcji. Funkcja wykonuje kolejne kroki buduj¹ce model z kolejnych warstw, które dostêpne s¹ jako gotowe komponenty z biblioteki Keras. Ka¿da z warstw jest parametryzowana wybranymi wartoœciami. 
Po utworzeniu modelu jest on kompilowany. Na tym etapie podawane s¹ równie¿ informacje odnoœnie tego jaki optymalizator, funkcja strat oraz metryka skutecznoœci ma zostaæ u¿yta. Dodatkowo tworzony jest równie¿ opis modelu wykorzystywany póŸniej w procesie monitorowania.

Na listingu \ref{lst:model-example} zosta³a zaprezentowana funkcja tworz¹ca przyk³adowy model splotowej sieci neuronowej z 1-wymiarowym filtrem.

\begin{lstlisting}[caption=Tworzenie przyk³adowego modelu, language=Python, label={lst:model-example}]
def conv_1D_with_adam(input_shape):
    # opis modelu tworzony na podstawie nazwy funkcji
    description = get_function_name()
    
    # specyfikacja modelu tworzonego sekwencyjnie
    model = Sequential()

    # dodanie warstwy splotowej z 32 1-wymiarowymi filtrami o rozmiarze 6 z funkcj¹ aktywacji 'relu'
    model.add(Conv1D(filters=32, kernel_size=6, padding='same', activation='relu', input_shape=input_shape))
    # warstwa max pooling'u o rozmiarze 2
    model.add(MaxPooling1D(pool_size=2))

    # warstwa zmniejszaj¹ca liczbê wymiarów danych
    model.add(Flatten())
    # warstwa typu 'fully-connected' o rozmiarze 64 neuronów
    model.add(Dense(64, activation='relu'))
    # warstwa typu 'fully-connected' z jednym neuronem i sigmoidaln¹ funkcj¹ aktywacji, która na wyjœciu zwróci wartoœæ '0' lub '1'
    model.add(Dense(1, activation='sigmoid'))

    # kompilacja modelu z u¿yciem optymalizatora Adam, funkcji strat binarnej entropii krzy¿owej oraz dok³adnoœci (accuracy) jako metryki
    model.compile(optimizer=Adam(),                  
                  loss='binary_crossentropy',
                  metrics=['acc'])
 
    return model, description
\end{lstlisting}

\subsection{Lista callback'ów}
\label{callbacks}
Dodatkowym parametrem przekazywanym do procesu uczenia jest lista tzw. \textit{callback}'ów, czyli wywo³añ zwrotnych, które umo¿liwiaj¹ otrzymywanie informacji z wnêtrza modelu podczas jego nauki.

Zosta³y utworzone 4 nastêpuj¹ce callbacki:
\begin{itemize}
\item \textbf{EarlyStopping} - pozwala na wczeœniejsze zatrzymanie procesu uczenia w przypadku, gdy monitorowana metryka nie zosta³a poprawiona przez okreœlon¹ iloœæ epok,
\item \textbf{LearningRateScheduler} - implementuje adaptacyjn¹ metodê zmiany wspó³czynnika uczenia wed³ug podanych regu³,
\item \textbf{ModelCheckpoint} - umo¿liwia zapisanie modelu, który posiada najlepsze dopasowanie wed³ug okreœlonej metryki,
\item \textbf{TensorBoard} - tworzy logi z procesu uczenia w podanym folderze, które mog¹ byæ u¿yte do monitorowania przez narzêdzie TensorBoard \ref{monitoring}.
\end{itemize}


Listing \ref{lst:callback} przedstawia implementacjê metod tworz¹cych \textit{callback}'i.

\begin{lstlisting}[caption=Tworzenie listy \textit{callback}'ów, language=Python, label={lst:callback}]
def step_decay(epoch):    
    initial_lrate=0.1
    drop=0.6
    epochs_drop = 10.0
    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
    
    return lrate


def callbacks_list(description): 
    return [
    callbacks.EarlyStopping(
        monitor='val_acc', 
        patience=30
    ),
    callbacks.LearningRateScheduler(step_decay),
        
    callbacks.ModelCheckpoint(
        filepath='tmp/best_model.h5', 
        monitor='val_acc',
        save_best_only=True
    ),
    callbacks.TensorBoard(
        log_dir='tmp/logs/{}. {}'.format(description, create_current_time()),
        histogram_freq=0,
        write_graph=True,
        write_images=True
    )
]
\end{lstlisting}

\subsection{Funkcja dopasowania}
\label{fit}
Wykonywana na modelu funkcja \textit{fit()} rozpoczyna procedurê uczenia, czyli dopasowania do danych.
Jako argumenty przykazywane s¹ do niej dane treningowe i walidacyjne, liczba epok ucz¹cych, \textit{callback}'i oraz inne parametry. Podczas nauki wyœwietlane s¹ informacje o postêpie uczenia oraz zwracany jest raport zawieraj¹cy historiê wartoœci metryk w kolejnych epokach, na podstawie których mo¿na np. narysowaæ wykres przedstawiaj¹cy postêpy.
Te wartoœci nie bêd¹ jednak u¿ywane, gdy¿ ca³a historia i postêpy uczenia s¹ ³adowane do programu monitoruj¹cego TensorBoard z logów tworzonych przez callback TensorBoard (patrz rozdzia³ \ref{monitoring}).
Dodatkowo dla ka¿dej iteracji zapisywany jest najlepszy model, który na sam koniec zostanie za³adowany i sprawdzony pod k¹tem skutecznoœci (patrz listing \ref{verify_accuracy}).

\begin{lstlisting}[caption=Wywo³anie metody \textit{fit()} na modelu, language=Python, label={lst:fit}]
history = model.fit(x_train_cv,
                    y_train_cv,
                    epochs=epochs,
                    batch_size=16,
                    callbacks=callbacks,
                    validation_data=(x_valid_cv, y_valid_cv),
                    verbose=0)
\end{lstlisting}

\subsection{Sprawdzenie skutecznoœci}
\label{verify_accuracy}
W celu sprawdzenia skutecznoœci wczytywany jest najlepiej dopasowany model z danej iteracji i jest on sprawdzany metod¹ \textit{evaluate()} na danych walidacyjnych. Wynik jest nastêpnie wyœwietlany i zapisywany do tablicy, która pos³u¿y do policzenia œredniej skutecznoœci modelu. Przedstawione dzia³ania wykonuje kod zaprezentowany na listingu \ref{lst:validation}

\begin{lstlisting}[caption=Sprawdzenie skutecznoœci najlepszego modelu, language=Python, label={lst:validation}]
model.load_weights("tmp/best_model.h5")
best_model_score.append(model.evaluate(x_valid_cv, y_valid_cv, batch_size=16, verbose=0))
        
print("--Best model validation accuracy: %.2f%%" % (best_model_score[fold_number][1]*100))
\end{lstlisting}


\section{Monitorowanie}
\label{monitoring}
Model sieci neuronowej po zakoñczeniu procesu nauki zwraca obiekt zawieraj¹cy raport z wynikami. W sytuacji, gdy proces uczenia przeprowadzany jest kilkukrotnie nale¿a³oby rêcznie zarz¹dzaæ obiektami raportów. Dodatkowo, przedstawienie wyników w sposób czytelny i ³atwy do zinterpretowania wymaga zaimplementowania metod wizualizacyjnych np. rysowanie wykresu. Nie ma jednak potrzeby rêcznej implementacji wy¿ej wymienionych funkcjonalnoœci, gdy¿ istniej¹ dedykowane narzêdzia umo¿liwiaj¹ce prezentacjê wyników w czasie rzeczywistym w przyjaznej dla u¿ytkownika formie.

Do monitorowania procesu uczenia zosta³o u¿yte narzêdzie \textit{TensorBoard}, które dostarczone jest razem z bibliotek¹ \textit{TensorFlow}. Pozwala ono m.in. na graficzn¹ wizualizacjê postêpów w procesie uczenia oraz automatyczne generowanie schematu modelu sieci neuronowej. Jest to narzêdzie uruchamiane w przegl¹darce, które wykorzystuje logi wygenerowane przez \textit{callback.TensorBoard} omawiany w rozdziale \ref{callbacks}. 

Na podstawowym widoku przedstawione s¹ 4 wykresy ilustruj¹ce nastêpuj¹ce wartoœci:
\begin{itemize}
\item \textbf{acc} - dok³adnoœæ w procesie uczenia,
\item \textbf{loss} - wartoœæ funkcji strat w procesie uczenia,
\item \textbf{val\_acc} - dok³adnoœæ w procesie walidacji,
\item \textbf{val\_loss} - wartoœæ funkcji strat w procesie walidacji.
\end{itemize}\
Wykresy zosta³y zaprezentowane na rysunku \ref{fig:tensor-board}. S¹ one interaktywe, wiêc mo¿na sprawdziæ dok³adn¹ wartoœæ w ka¿dym punkcie wykresu.
\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/tensor-board.png}
	\caption{Dynamicznie tworzone wykresy w programie TensorBoard}
	\label{fig:tensor-board}
\end{figure}

Wizualna reprezentacja utworzonego modelu równie¿ mo¿e byæ pomocna przy ocenie architektury tworzonej sieci neuronowej. Fragment przyk³adowego schematu wygenerowany przez TensorBoard na podstawie modelu utworzonego w kodzie pokazany zosta³ na rysunku \ref{fig:tensor-board-model}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=10cm]{Rysunki/Rozdzial5/tensor-board-model.png}
	\caption{Fragment schematu modelu wygenerowany przez TensorBoard}
	\label{fig:tensor-board-model}
\end{figure}

TensorBoard udostêpnia równie¿ wiele innych funkcjonalnoœci, które nie zosta³y u¿yte jak np. rysowanie wykresów w³asnorêcznie zdefiniowanych metryk, monitorowanie i wizualizacjê rozk³adu danych w grafie itp. Wiêcej informacji na temat mo¿liwoœci TensorBoard mo¿na znaleŸæ w dokumentacji \cite{tensor_board}. 

\section{Wybór rodzaju sieci neuronowej}
Wybór rodzaju oraz wstêpnej architektury sieci neuronowej ma kluczowe znaczenie. To w³aœnie te czynniki bêd¹ w g³ównej mierze decydowaæ o skutecznoœci modelu. Po dobraniu rodzaju sieci neuronowej, który umo¿liwia uzyskanie jak najlepszych wyników nastêpuje faza optymalizacji, czyli dostosowywania parametrów, która zostanie opisana w rozdziale \ref{optimization}.

Przy wyborze modelu warto zacz¹æ od rozwi¹zañ najprostszych. Proste modele bêd¹ potrzebowa³y zdecydowanie mniej czasu na naukê, ni¿ te z³o¿one, dlatego dziêki u¿yciu tego podejœcia bêdzie mo¿na w krótkim czasie uzyskaæ pierwsze rezultaty. Mo¿e siê równie¿ okazaæ, ¿e bardzo prosty model jest dostatecznie skuteczny w aktualnie rozpatrywanym problemie.

Podczas próby znalezienia odpowiedniego rodzaju sieci neuronowej rozpatrzone zosta³y nastêpuj¹ce opcje:
\begin{itemize}
\item klasyczna sieæ typu \textit{fully-connected},
\item sieæ splotowa z filtrem 1-wymiarowym,
\item sieæ splotowa z filtrem 2-wymiarowym,
\item sieæ rekurencyjna,
\item po³¹czenie sieci splotowej z rekurencyjn¹.
\end{itemize}

\subsection{Klasyczna sieæ neuronowa typu fully connected}
Pierwszym, najprostszym rozwi¹zaniem, które zosta³o przetestowane jest klasyczna sieæ neuronowa typu \textit{fully connected}. W bibliotece Keras warstwy tworz¹ce tak¹ sieæ nosz¹ nazwê \textit{Dense}.
Model pocz¹tkowy, który zosta³ stworzony zaprezentowano na listingu \ref{lst:fully-connected}. 

\begin{lstlisting}[caption=Model sieci neuronowej typu \textit{fully connected}, language=Python, label={lst:fully-connected}]
model = Sequential()
model.add(Flatten(input_shape=input_shape))
model.add(Dense(1000, kernel_initializer='normal', activation='relu'))
model.add(Dense(30, kernel_initializer='normal', activation='relu'))
model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))
    
sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)
model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])
\end{lstlisting}

Sieæ sk³ada siê z warstwy wejœciowej \textit{Flatten}, która odpowiada za zmniejszenie stopnia wymiarowoœci danych tak, aby mog³y byæ u¿yte w sieci tego typu. Przez tê operacjê tracone s¹ pewne informacje o s¹siedztwie danych, gdy¿ teraz s¹ one jedynie wektorem zamiast wielowymiarow¹ tablic¹, jednak jest ona wymagana.
Kolejne warstwy stanowi¹ warstwy \textit{Dense} zawieraj¹ce ró¿n¹ iloœæ neuronów. Pocz¹tkowo zosta³y u¿yte wartoœci 1000 i 30.
Sieæ zakoñczona jest warstw¹ z jednym neuronem i sigmoidaln¹ funkcj¹ aktywacji, która zwraca na wyjœciu wartoœæ 1 lub 0.
Jako optymalizator u¿yty zosta³ \textit{SGD} (\textit{ang. stochastic gradient descent}) z funkcj¹ strat binarnej entropii krzy¿owej \textit{(ang. binary crossentropy)}. Podsumowanie modelu wygenerowane przez bibliotekê \textit{Keras}, opisuj¹ce iloœæ warstw oraz ich wielkoœci, zamieszczono na listingu \ref{lst:summary_fc}.

\begin{lstlisting}[caption=Podsumowanie modelu sieci neuronowej typu \textit{fully connected}, language=Python, label={lst:summary_fc}]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_3 (Flatten)          (None, 3200)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 1000)              3201000   
_________________________________________________________________
dense_4 (Dense)              (None, 30)                30030     
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 31        
=================================================================
Total params: 3,231,061
Trainable params: 3,231,061
Non-trainable params: 0
____________________________
\end{lstlisting}

Proces nauki zaproponowanej sieci neuronowej przebiega³ szybko ze wzglêdu na jej prostotê. Jedna pe³na iteracja trwa³a jedynie oko³o 90 sekund. Wyniki by³y jednak bardzo s³abe. Skutecznoœæ podczas uczenia modelu oscylowa³a wokó³ wartoœci 50\% (48\% - 52\%). Skutecznoœæ walidacyjna równie¿ by³a praktycznie sta³a i zawsze bliska \textbf{50\%} \ref{fig:fully-connected-results}. 

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/fully-connected-results.png}
	\caption{Wyniki skutecznoœci sieci typu \textit{fully-connected}}
	\label{fig:fully-connected-results}
\end{figure}

Mo¿na wiêc zauwa¿yæ, ¿e sieæ dzia³a³a praktycznie w sposób losowy, gdy¿ tyle wynosi w³aœnie prawdopodobieñstwo przewidzenia wyniku z poœród dwóch wartoœci. Pomimo próby modyfikacji sieci poprzez zwiêkszenie/zmniejsznie liczby warstw, zwiêkszenie/zmniejszenie liczby neuronów, zmiany optymalizatora i funkcji strat, wci¹¿ przynosi³a ona podobne wyniki. Nie uda³o siê osi¹gn¹æ lepszych rezultatów dla tego typu sieci neuronowej, dlatego nie by³a ona wiêcej wykorzystywana. Okaza³a siê niewystarczaj¹ca do rozwi¹zania rozpatrywanego problemu.


\subsection{Sieæ splotowa z filtrem 1-wymiarowym}
\label{conv-1d}
Kolejnym testowanym typem by³a sieæ splotowa z 1-wymiarowym filtrem, której zastosowanie polecane jest do danych w formie szeregów czasowych np. dane z ¿yroskopu czy akcelerometru. W bibliotece Keras warstwy tego typu nazwane s¹ \textit{Conv1D (ang. Convolutional 1-dimension)}. Filtr jednowymiarowy jest w stanie odnajdywaæ podobieñstwa miêdzy danymi jedynie w jednym wymiarze. W przypadku dostêpnych danych z EEG mog¹ byæ to wartoœci tylko z jednego kana³u jednoczeœnie. Oznacza to, ¿e byæ mo¿e sieæ tego typu nie bêdzie w stanie zauwa¿yæ zale¿noœci pomiêdzy danymi znajduj¹cymi siê na ró¿nych kana³ach.

Wstêpny model sieci, który zosta³ wybrany przedstawiony zosta³ na listingu \ref{lst:conv-1d}. Sk³ada siê on z dwóch warstw konwolucyjnych o odpowiednio 32 i 16 filtrach z warstwami \textit{max pooling}'u. Sieci splotowe zwykle zakoñczone s¹ kilkoma warstwami typu \textit{fully-connected}. W tym wypadku zosta³a zastosowana jedna warstwa z 64 neuronami oraz warstwa wyjœciowa z jednym neuronem i sigmoidaln¹ funkcj¹ aktywacji. Warstwa koñcowa tego typu bêdzie wykorzystywana w ka¿dym modelu, gdy¿ daje ona mo¿liwoœæ zwrócenia na wyjœciu z sieci wartoœæ 0 lub 1, które oznaczaj¹ brak lub pojawienie siê ataku. Podsumowanie modelu znajduje siê na listingu \ref{lst:conv1d_summary}

\begin{lstlisting}[caption=Model splotowej sieci neuronowej z 1-wymiarowym filtrem, language=Python, label={lst:conv-1d}]
model = Sequential()

model.add(Conv1D(filters=32, kernel_size=6, padding='same', activation='relu', input_shape=input_shape))
model.add(MaxPooling1D(pool_size=2))
    
model.add(Conv1D(filters=16, kernel_size=6, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
    
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['acc'])
\end{lstlisting}

\begin{lstlisting}[caption=Podsumowanie modelu splotowej sieci neuronowej z 1-wymiarowym filtrem, language=Python, label={lst:conv1d_summary}]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_2 (Conv1D)            (None, 200, 32)           3104      
_________________________________________________________________
dropout_5 (Dropout)          (None, 200, 32)           0         
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 100, 32)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 100, 16)           3088      
_________________________________________________________________
dropout_6 (Dropout)          (None, 100, 16)           0         
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 50, 16)            0         
_________________________________________________________________
flatten_5 (Flatten)          (None, 800)               0         
_________________________________________________________________
dense_9 (Dense)              (None, 64)                51264     
_________________________________________________________________
dense_10 (Dense)             (None, 1)                 65        
=================================================================
Total params: 57,521
Trainable params: 57,521
Non-trainable params: 0
__________________________
\end{lstlisting}

Skonstruowana w ten sposób sieæ neuronowa uczy³a siê co prawda d³u¿ej, ni¿ sieæ typu \textit{fully-connected}, gdy¿ czas nauki wynosi³ oko³o 240 sekund, jednak wyniki by³y o wiele lepsze i oscylowa³y w okolicach \textbf{70\%}. Jest to bardzo dobry wynik w porównaniu do poprzedniego bior¹c pod uwagê prost¹ architekturê sieci. Sieæ jednak bardzo szybko siê przeucza³a \textit{(ang. overfitting)}. Mo¿na to zauwa¿yæ po osi¹gniêciu 100\% skutecznoœci podczas procesu nauki \textit{(acc)} oraz wzroœcie wartoœci funkcji strat podczas walidacji \textit{(val\_loss)} (patrz rysunek \ref{fig:conv-1d-results}).

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/conv-1d-results.png}
	\caption{Wyniki prostej sieci splotowej z 1-wymiarowym filtrem}
	\label{fig:conv-1d-results}
\end{figure}

Wyniki by³y obiecuj¹ce, wiêc sieæ tego typu zosta³a wziêta pod uwagê w dalszych pracach polegaj¹cych na optymalizacji w celu uzyskania jak najlepszych wyników (rozdzia³ \ref{optimization}).

\subsection{Sieæ splotowa z filtrem 2-wymiarowym}
Nastêpnym typem sieci, która zosta³a przetestowana by³a sieæ splotowa z filtrem 2-wymiarowym \textit{(Conv2D)}. Sieæ ta jest bardzo podobna do konwolucyjnej sieci jednowymiarowej opisywanej w poprzednim podrozdziale \ref{conv-1d}. Ró¿nic¹ jest typ zastosowanego filtra, który w tym przypadku jest 2-wymiarowy. Pozwala wiêc analizowaæ dane w dwuwymiarowej przestrzeni. Sieci tego typu u¿ywane s¹ przewa¿nie do rozpoznawania obrazów, jednak równie dobrze mog¹ siê sprawdziæ te¿ na danych innego typu. W przypadku EEG sieæ z 2-wymiarowym filtrem bêdzie w stanie analizowaæ szereg czasowy obejmuj¹c kilka kana³ów jednoczeœnie, dziêki temu mo¿liwe bêdzie zauwa¿enie powi¹zania pomiêdzy danymi z innych kana³ów.

Analogicznie do modelu sieci z filtrem jednowymiarowym zosta³ stworzony podobny z 2-wymiarowym filtrem (patrz listing \ref{lst:conv-2d}).
Podsumowanie modelu zaprezentowano na listingu \ref{lst:conv2d_summary}.

\begin{lstlisting}[caption=Model splotowej sieci neuronowej z 2-wymiarowym filtrem, language=Python, label={lst:conv-2d}, float=!h]
model = Sequential()

model.add(Conv2D(32,(3,3),strides = (1,1),padding='same', activation = 'relu', input_shape=input_shape))
model.add(MaxPooling2D((2,2)))
    
model.add(Conv2D(16,(3,3),strides = (1,1),name='conv3', activation = 'relu'))
model.add(MaxPooling2D((2,2)))
    
model.add(Flatten())
model.add(Dense(64,activation = 'relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['acc'])
\end{lstlisting}

\begin{lstlisting}[caption=Podsumowanie modelu splotowej sieci neuronowej z 2-wymiarowym filtrem, language=Python, label={lst:conv2d_summary}, float=!h]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 200, 16, 32)       320       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 100, 8, 32)        0         
_________________________________________________________________
conv3 (Conv2D)               (None, 98, 6, 16)         4624      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 49, 3, 16)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 2352)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                150592    
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 155,601
Trainable params: 155,601
Non-trainable params: 0
_________________________________________________________________
\end{lstlisting}

Nauka tego modelu sieci trwa³a trochê d³u¿ej, ni¿ poprzedniego, gdy¿ czas wynosi³ oko³o 300 sekund. Okaza³o siê jednak, ¿e sieæ która ró¿ni siê od poprzedniej jedynie wymiarem filtra osi¹gnê³a skutecznoœæ na poziomie \textbf{75\%}. 
Po wynikach zaprezentowanych na rysunku \ref{fig:conv-2d-results} mo¿na jednak zauwa¿yæ, ¿e pomimo lepszej skutecznoœci sieæ tego typu równie¿ bardzo szybko ulega przeuczeniu. Problem ten bêdzie rozwi¹zywany podczas próby jej optymalizacji w rozdziale \ref{optimization}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/conv-2d-results.png}
	\caption{Wyniki prostej sieci splotowej z 2-wymiarowym filtrem}
	\label{fig:conv-2d-results}
\end{figure}

\subsection{Sieæ rekurencyjna}
W nastêpnym kroku przetestowana zosta³a rekurencyjna sieæ neuronowa. Pocz¹tkowy model sieci, który zosta³ utworzony zaprezentowany zosta³ na listingu \ref{lst:recursive}. Podsumowanie modelu zamieszczono na listingu \ref{lst:rec_summary}.

\begin{lstlisting}[caption=Model rekurencyjnej sieci neuronowej, language=Python, label={lst:recursive}, float=!h]
model = Sequential()

model.add(LSTM(100, input_shape=input_shape))
model.add(Dense(1, activation='sigmoid'))
    
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
\end{lstlisting}

\begin{lstlisting}[caption=Podsumowanie modelu rekurencyjnej sieci neuronowej, language=Python, label={lst:rec_summary}, float=!h]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_2 (LSTM)                (None, 100)               46800     
_________________________________________________________________
dense_17 (Dense)             (None, 1)                 101       
=================================================================
Total params: 46,901
Trainable params: 46,901
Non-trainable params: 0
_________________________________________________________________
\end{lstlisting}

Sk³ada siê ona z jednej warstwy typu \textit{LSTM (ang. Long Short-Term Memory)}. Sieæ mimo prostej budowy uczy siê bardzo d³ugo. Czas pe³nej iteracji nauki wynosi a¿ 5700 sekund. Wynika to ze z³o¿onoœci sposobu funkcjonowania tego typu sieci. Pomimo d³u¿szego czasu nauki zaproponowana sieæ osi¹ga skutecznoœæ na poziomie jedynie ok. \textbf{60\%} \ref{fig:recursive-results}. 

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/recursive-results.png}
	\caption{Wyniki prostej sieci rekurencyjnej}
	\label{fig:recursive-results}
\end{figure}

Jest to wynik o wiele gorszy w porównaniu do sieci konwolucyjnych testowanych w poprzednich rozdzia³ach. Próby rozbudowy i prostej optymalizacji sieci nie przynosi³y po¿¹danych skutków, dlatego zosta³a ona pominiêta w dalszych badaniach.
 
Poczyniono jednak próby po³¹czenia tego typu sieci z sieciami konwolucyjnymi. Wyniki przeprowadzonych prób opisane zosta³y w rozdziale \ref{cnn_lstm}

\subsection{Po³¹czenie sieci splotowej z rekurencyjn¹}
\label{cnn_lstm}
Ostatnim typem sieci, którego skutecznoœæ zosta³a sprawdzona jest po³¹czenie sieci splotowej z rekurencyjn¹ tzw. \textit{CNN LSTM}. Jest to bardziej rozbudowana stuktura sieci polegaj¹ca na u¿yciu kilku warstw konwolucyjnych zakoñczonych warstwami rekurencyjnymi. Prosty model sieci zbudowany wed³ug tej koncepcji zaprezentowany zosta³ na listingu \ref{lst:cnn_lstm}. Jego podsumowanie zamieszczono na listingu \ref{lst:cnn_lstm_summary}.

\begin{lstlisting}[caption=Model sieci typu CNN LSTM, language=Python, label={lst:cnn_lstm}, float=!h]
model = Sequential()

model.add(Conv1D(filters=64, kernel_size=6, padding='same', activation='relu', input_shape=input_shape))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(filters=32, kernel_size=6, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
    
model.add(LSTM(100))
    
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
    
model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['acc'])
\end{lstlisting}

\begin{lstlisting}[caption=Podsumowanie modelu sieci typu CNN LSTM, language=Python, label={lst:cnn_lstm_summary}, float=!h]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_2 (Conv1D)            (None, 200, 32)           3104      
_________________________________________________________________
dropout_1 (Dropout)          (None, 200, 32)           0         
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 100, 32)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 100, 16)           3088      
_________________________________________________________________
dropout_2 (Dropout)          (None, 100, 16)           0         
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 50, 16)            0         
_________________________________________________________________
lstm_4 (LSTM)                (None, 100)               46800     
_________________________________________________________________
dense_19 (Dense)             (None, 128)               12928     
_________________________________________________________________
dense_20 (Dense)             (None, 1)                 129       
=================================================================
Total params: 66,049
Trainable params: 66,049
Non-trainable params: 0
_________________________________________________________________
\end{lstlisting}

Pomimo bardziej z³o¿onej struktury proces nauki utworzonej sieci przebiega³ krócej, ni¿ w przypadku tej sk³adaj¹cej siê jedynie z warstwy rekurencyjnej i wynosi³ oko³o 1800 sekund. Spowodowane jest to tym, ¿e dane wejœciowe ulegaj¹ zmniejszeniu za pomoc¹ warstw konwolucyjnych i wstêpnie przetworzone trafiaj¹ na warstwê rekurencyjn¹. Dziêki temu warstwa rekurencyjna potrzebuje mniej czasu do nauki, ni¿ w przypadku, gdy podawane jej s¹ nieprzygotowane dane.

Osi¹gniête wyniki by³y co prawda lepsze od sieci rekurencyjnej, jednak nie przewy¿sza³y wyników modeli zbudowanych z u¿yciem warstw konwolucyjnych. Osi¹gniête wyniki waha³y siê w granicach \textbf{65\%} \ref{fig:cnn_lstm}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/cnn-lstm-results.png}
	\caption{Wyniki sieci konwoluncyjnej po³¹czonej z rekurencyjn¹}
	\label{fig:cnn_lstm}
\end{figure}
Zosta³y podjête próby roszerzenia modelu sieci oraz zmiany parametrów, jednak wyniki nie zosta³y w znacz¹cym stopniu poprawione. Ze wzglêdu na to sieæ tego typu nie zosta³a u¿yta w dalszych badaniach.


\subsection{Podsumowanie}
Podsumowanie wyników testowania skutecznoœci poszczególnych typów sieci zosta³y zaprezentowane w tabeli \ref{tbl:comparison}. 

\begin{table}[h!]
	\caption{Porównanie czasów nauki oraz skutecznoœci poszczególnych typów sieci}
	\label{tbl:comparison}
	\begin{tabular}{|l|l|l|}
		\hline
		\textbf{Nazwa}  & \textbf{Œredni czas jednej iteracji {[}s{]}} & \textbf{Œrednia skutecznoœæ {[}\%{]}} \\ \hline
		Fully connected & 90                                           & 50                                    \\ \hline
		CNN 1D          & 240                                          & 70                                    \\ \hline
		CNN 2D          & 300                                          & 75                                    \\ \hline
		LSTM            & 5700                                         & 60                                  \\ \hline
		CNN LSTM        & 1800                                         & 65                                  \\ \hline
	\end{tabular}
\end{table}

Mo¿na zauwa¿yæ, ¿e najlepsze wyniki i stosunkowo niedu¿e czasy nauki osi¹gnê³a sieæ splotowa z filtrem 2-wymiarowym. Model tej sieci zosta³ wiêc wybrany jako ten najlepiej przystosowany do rozwi¹zywania badanego problemu. Próba i sposoby optymalizacji wyników tego modelu zosta³y opisane w kolejnym rozdziale \ref{optimization}.


\section{Optymalizacja}
\label{optimization}
Sieæ typu konwolucyjnego z filtrem 2-wymiarowym osi¹ga³a najlepsze wyniki wœród testowanych prototypów. To w³aœnie tej architekturze zosta³o wiêc poœwiêcone najwiêcej pracy maj¹cej na celu optymalizacjê i polepszenie otrzymywanych wyników. 
Wyniki ju¿ nawet przy zaprezentowanym prostym modelu by³y obiecuj¹ce, gdy¿ sieæ osi¹ga³a œrednio 75\% skutecznoœci.

Najwiêkszym i najczêœciej wystêpuj¹cym problemem podczas nauki sieci neuronowych jest przeuczenie \textit{(ang. overfitting)}, które polega na zbytnim dopasowaniu siê modelu do danych ucz¹cych. Przeuczony model bêdzie posiada³ zbyt ma³¹ umiejêtnoœæ generalizacji, w wyniku czego bêdzie bardzo dobrze potrafi³ klasyfikowaæ dane, które do tej pory widzia³, jednak nie poradzi sobie zbyt dobrze z zupe³nie nowymi.
Overfitting zwykle objawia siê osi¹gniêciem skutecznoœci bliskiej 100\% i wartoœci funkcji strat wynosz¹cej oko³o 0 podczas procesu uczenia. Skutkiem tego s¹ oczywiœcie o wiele gorsze wyniki podczas procesu walidacji. Tak jest równie¿ w przypadku tego prostego modelu, co mo¿na zaobserwowaæ na wykresach prezentuj¹cych przebieg uczenia przedstawiony na rysunku \ref{fig:cnn_start}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/cnn-start.png}
	\caption{Wyniki modelu pocz¹tkowego konwolucyjnej sieci neuronowej}
	\label{fig:cnn_start}
\end{figure}

Istnieje kilka sposobów przeciwdzia³ania nadmiernego dopasowania. Zostan¹ one opisane w kolejnych rozdzia³ach.

\subsection{Batch normalization}
Pierwszym sposobem jest wykorzystanie \textit{batch normalization}. Technika ta polega na normalizacji wartoœci wyjœciowych z danej warstwy tak, aby mia³y one œredni¹ 0 i odchylenie standardowe 1. Jest to zabieg podobny do tego przeprowadzonego podczas przygotowywania danych do procesu nauki z t¹ ró¿nic¹, ¿e mo¿na go stosowaæ dla wartoœci wyjœciowych z warstw. Dodatkowo zmniejsza zale¿noœæ wyników osi¹ganych przez sieæ od wartoœci, którymi zainicjalizowane by³y wagi oraz poprawia przep³yw gradientu. Wiêcej informacji dotycz¹cej \textit{batch normalization} mo¿na znaleŸæ w publikacji poœwiêconej temu zagadnieniu \cite{batch_normalization}.

W bibliotece \textit{Keras} dostêpne s¹ gotowe warstwy o nazwie \textit{BatchNormalization} dostarczaj¹ce opisan¹ funkcjonalnoœæ. Wystarczy dodaæ warstwê poprzedzaj¹c¹ funkcjê aktywacji \ref{lst:cnn_batch_normalization}.

\begin{lstlisting}[caption=Model sieci konwolucyjnej z warstwami BatchNormalization, language=Python, label={lst:cnn_batch_normalization}]
model = Sequential()

model.add(Conv2D(32, (3,3), strides = (1,1), padding='same', input_shape=input_shape))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(16, (3,3), strides = (1,1), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D((2,2)))

model.add(Flatten())
model.add(Dense(64, activation = 'relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['acc'])
\end{lstlisting}

Wykonanie tak prostej operacji pozwoli³o na zwiêkszenie œredniej skutecznoœci sieci do \textbf{77\%}. Problem \textit{overfitting}'u co prawda nie zosta³ ca³kowicie rozwi¹zany, jednak mo¿na zauwa¿yæ drobne postêpy wzglêdem poprzedniej wersji modelu \ref{fig:cnn_batch_normalization}

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/cnn-batch-normalization.png}
	\caption{Wyniki modelu konwolucyjnej sieci neuronowej z warstwami \textit{BatchNormalization}}
	\label{fig:cnn_batch_normalization}
\end{figure}

\subsection{Dropout}
Kolejn¹ technik¹ jest wykorzystanie tzw. \textit{dropout}'u. Polega ona na tym, ¿e wartoœci na warstwie wyjœciowej s¹ zerowane ze wskazanym prawdopodobieñstwem. Dziêki temu podczas ka¿dej epoki ucz¹cej dane wygl¹daj¹ trochê inaczej, ni¿ poprzednio, dlatego sieæ nie uczy siê zawsze na podstawie tych samych danych, tylko ich drobnych modyfikacjach. W bibliotece \textit{Keras} zosta³a zaimplementowana gotowa warstwa \textit{Dropout}, która umo¿liwia wykorzystanie tej techniki \ref{lst:cnn_dropout}. Zosta³a ona na³o¿ona na wyniki warstwy typu \textit{Dense}.

\begin{lstlisting}[caption=Model sieci konwolucyjnej z warstwami Dropout, language=Python, label={lst:cnn_dropout}]
model = Sequential()

model.add(Conv2D(32, (3,3), strides = (1,1), padding='same', input_shape=input_shape))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(16, (3,3), strides = (1,1), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D((2,2)))

model.add(Flatten())
model.add(Dense(64, activation = 'relu'))
model.add(Dropout(0.25))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['acc'])
\end{lstlisting}

Po dodaniu \textit{dropout}'u wyniki nieznacznie siê poprawi³y do wartoœci œrednio \textbf{78\%} oraz zmniejszony zosta³ problem przeuczenia. Podczas uczenia nie jest osi¹gana ju¿ skutecznoœæ 100\% oraz wartoœci funkcji strat równej 0. Podczas walidacji wartoœæ funkcji strat dalej roœnie, jednak w mniejszym stopniu, ni¿ poprzednio \ref{fig:cnn_dropout}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/cnn-dropout.png}
	\caption{Wyniki modelu konwolucyjnej sieci neuronowej z warstwami \textit{Dropout}}
	\label{fig:cnn_dropout}
\end{figure}

\subsection{Wybór optymalizatora}
Kolejnym czynnikiem wp³ywaj¹cym na skutecznoœæ sieci jest wybrany optymalizator. Jego zadaniem jest modyfikacja wag na podstawie wartoœci funkcji strat.
W bibliotece \textit{Keras} istnieje wiele gotowych optymalizatorów, które mog¹ zostaæ wybrane podczas kompilacji modelu. Najbardziej popularnymi s¹: 
\textit{Adam}, \textit{RMSProp} oraz \textit{SGD (ang. Stochastic gradient descent)}.
Wyniki osi¹gniête przy u¿yciu poszczególnych optymalizatorów zamieszczone zosta³y w tabeli \ref{tbl:optimizers}.

\begin{table}[h!]
	\caption{Porównanie skutecznoœci sieci przy u¿yciu poszczególnych optymalizatorów}
	\label{tbl:optimizers}
\begin{tabular}{|l|l|}
\hline
\textbf{Optymalizator} & \textbf{Œrednia skutecznoœæ {[}\%{]}} \\ \hline
Adam                   & 78                                    \\ \hline
SGD                    & 79                                    \\ \hline
RMSProp                & 76,5                                      \\ \hline
\end{tabular}
\end{table}

Optymalizatorem, który osi¹ga³ najlepsze wyniki by³ SGD. Dodatkowo optymalizator tego typu bardzo dobrze sprawdza³ siê równie¿ w parze z adaptacyjnie dobieranym wspó³czynnikiem uczenia \ref{adaptive_learning_rate}.

\subsection{Adaptacyjny wspó³czynnik uczenia}
\label{adaptive_learning_rate}
Nastêpnym krokiem jest dodanie mechanizmu, który pozwoli na adaptacyjny dobór wspó³czynnika uczenia. Sieæ przeucza siê, gdy¿ w póŸniejszych epokach uczy siê zbyt intensywnie istniej¹cych ju¿ reprezentacji danych. Wspó³czynnik uczenia kontroluje stopieñ nauki sieci i mo¿e byæ on ustawiany dynamicznie. Za pomoc¹ wczeœniej opisywanych \textit{callback}'ów dodany zosta³ \textit{LearningRateScheduler}, który bêdzie obni¿a³ wspó³czynnik uczenia wraz z postêpem epok ucz¹cych.
Listing \ref{lst:learning_rate_scheduler} przedstawia regu³ê, wed³ug której bêdzie on modyfikowany.

\begin{lstlisting}[caption=Regu³a modyfikacji wspó³czynnika uczenia, language=Python, label={lst:learning_rate_scheduler}, float=!h]
def step_decay(epoch):    
    initial_lrate=0.1
    drop=0.6
    epochs_drop = 10.0
    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
    
    return lrate
\end{lstlisting}

Dynamiczna zmiana wspó³czynnika uczenia pozwoli³a na czêœciowe zredukowanie nadmiernego dopasowania sieci, jednak wci¹¿ zaobserwowaæ mo¿na jej przeuczanie. Mimo to skutecznoœæ zosta³a poprawiona do blisko \textbf{80,5\%} \ref{fig:adaptive_learning_rate}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=14cm]{Rysunki/Rozdzial5/conv-adaptive-rate.png}
	\caption{Wyniki modelu po dodaniu zmiennego wspó³czynnika uczenia}
	\label{fig:adaptive_learning_rate}
\end{figure}

\subsection{Modyfikacja rozmiaru sieci}
Ostatnim wykonanym krokiem by³a modyfikacja rozmiaru sieci oraz poszczególnych warstw. Wprowadzanych by³o wiele zmian, na podstawie których mo¿na by³o zaobserwowaæ, ¿e zbyt ma³y rozmiar sieci zmniejsza³ skutecznoœæ modelu, gdy¿ nie mia³ on wtedy wystarczaj¹cej przestrzeni do zapamiêtania wszystkich wa¿nych informacji. Rozbudowywanie modelu pocz¹tkowo zwiêksza³o skutecznoœæ, jednak po osi¹gniêciu pewnego progu dodawanie kolejnych neuronów oraz warstw przynosi³o jedynie negatywne skutki.

Listing \ref{lst:conv_final} zawiera model, który podczas testów osi¹gn¹³ najlepsze rezultaty. Zawiera on 3 warstwy konwolucyjne z kolejno 64, 64 i 32 filtrami o rozmiarze 3x3. Po ka¿dej z warstw zastosowano warstwê \textit{BatchNormalization}, funkcjê aktywacji \textit{relu} oraz warstwê \textit{max pooling}'u z filtrem o rozmiarze 2x2. W nastêpstwie warstw konwolucyjnych sieæ posiada 3 warstwy typu \textit{fully connected} rozdzielonych warstwami \textit{Dropout}. Warstwy posiadaj¹ rozmiar 64 oraz 32 neurony z warstw¹ wyjœciow¹ sk³adaj¹c¹ siê z jednego neuronu i sigmoidaln¹ funkcj¹ aktywacji \textit{(ang. sigmoid)}, która zwraca na wyjœciu wartoœæ 0 lub 1. Model skompilowany zosta³ przy u¿yciu optymalizatora \textit{SGD} i jako funkcja strat u¿yta zosta³a \textit{binary crossentropy}. 

\begin{lstlisting}[caption=Ostateczny model sieci konwolucyjnej, language=Python, label={lst:conv_final}, float=h! ]
model = Sequential()
    
model.add(Conv2D(64,(3,3),strides = (1,1),name='layer_conv1',padding='same', input_shape=input_shape))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D((2,2),name='maxPool1'))

model.add(Conv2D(64,(3,3),strides = (1,1),name='layer_conv2',padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D((2,2),name='maxPool2'))

model.add(Conv2D(32,(3,3),strides = (1,1),name='conv3',padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D((2,2),name='maxPool3'))

model.add(Flatten())
model.add(Dense(64,activation = 'relu',name='fc0'))
model.add(Dropout(0.25))
model.add(Dense(32,activation = 'relu',name='fc1'))
model.add(Dropout(0.25))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=SGD(lr=0.01, momentum=0.5, decay=0.0, nesterov=False), 
              loss='binary_crossentropy',
              metrics=['acc'])
\end{lstlisting}

Podsumowanie modelu wygenerowane za pomoc¹ funkcji \textit{model.summary()} pokazuj¹ce u¿yte warstwy, rozmiary wyjœciowe oraz iloœæ parametrów przedstawia siê nastêpuj¹co \ref{lst:conv_summary}.

\begin{lstlisting}[caption=Podsumowanie modelu, language=Python, label={lst:conv_summary}]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
layer_conv1 (Conv2D)         (None, 200, 16, 64)       640       
_________________________________________________________________
batch_normalization_88 (Batc (None, 200, 16, 64)       256       
_________________________________________________________________
activation_88 (Activation)   (None, 200, 16, 64)       0         
_________________________________________________________________
maxPool1 (MaxPooling2D)      (None, 100, 8, 64)        0         
_________________________________________________________________
layer_conv2 (Conv2D)         (None, 100, 8, 64)        36928     
_________________________________________________________________
batch_normalization_89 (Batc (None, 100, 8, 64)        256       
_________________________________________________________________
activation_89 (Activation)   (None, 100, 8, 64)        0         
_________________________________________________________________
maxPool2 (MaxPooling2D)      (None, 50, 4, 64)         0         
_________________________________________________________________
conv3 (Conv2D)               (None, 50, 4, 32)         18464     
_________________________________________________________________
batch_normalization_90 (Batc (None, 50, 4, 32)         128       
_________________________________________________________________
activation_90 (Activation)   (None, 50, 4, 32)         0         
_________________________________________________________________
maxPool3 (MaxPooling2D)      (None, 25, 2, 32)         0         
_________________________________________________________________
flatten_30 (Flatten)         (None, 1600)              0         
_________________________________________________________________
fc0 (Dense)                  (None, 64)                102464    
_________________________________________________________________
dropout_59 (Dropout)         (None, 64)                0         
_________________________________________________________________
fc1 (Dense)                  (None, 32)                2080      
_________________________________________________________________
dropout_60 (Dropout)         (None, 32)                0         
_________________________________________________________________
dense_30 (Dense)             (None, 1)                 33        
=================================================================
Total params: 161,249
Trainable params: 160,929
Non-trainable params: 320
\end{lstlisting}

Na listingu \ref{lst:results} zamieszczone zosta³y logi zawieraj¹ce wyniki skutecznoœci sieci najlepszych modeli z poszczególnych \textit{fold}'ów zebrane podczas trzech iteracji. Œrednia skutecznoœæ modelu to oko³o \textbf{82\%} (\textit{grand mean of average accuracy}). 

\begin{lstlisting}[caption=Wyniki skutecznoœci, language=Python, label={lst:results}]
Iteration 1

Fold:  0
--Best model validation accuracy: 79.65%

Fold:  1
--Best model validation accuracy: 83.93%

Fold:  2
--Best model validation accuracy: 83.04%

Fold:  3
--Best model validation accuracy: 84.82%

Fold:  4
--Best model validation accuracy: 86.61%

Fold:  5
--Best model validation accuracy: 77.68%

Fold:  6
--Best model validation accuracy: 81.08%

Fold:  7
--Best model validation accuracy: 78.38%

Fold:  8
--Best model validation accuracy: 81.98%

Fold:  9
--Best model validation accuracy: 83.78%


Best models average validation accuracy: 0.820943
Best models standard deviation of accuracy: 0.027431
Iteration 1 time: 511.17 seconds


Iteration 2

Fold:  0
--Best model validation accuracy: 78.76%

Fold:  1
--Best model validation accuracy: 83.04%

Fold:  2
--Best model validation accuracy: 83.93%

Fold:  3
--Best model validation accuracy: 84.82%

Fold:  4
--Best model validation accuracy: 87.50%

Fold:  5
--Best model validation accuracy: 75.89%

Fold:  6
--Best model validation accuracy: 77.48%

Fold:  7
--Best model validation accuracy: 83.78%

Fold:  8
--Best model validation accuracy: 78.38%

Fold:  9
--Best model validation accuracy: 84.68%


Best models average validation accuracy: 0.818264
Best models standard deviation of accuracy: 0.036667
Iteration 2 time: 527.52 seconds


Iteration 3

Fold:  0
--Best model validation accuracy: 81.42%

Fold:  1
--Best model validation accuracy: 85.71%

Fold:  2
--Best model validation accuracy: 83.93%

Fold:  3
--Best model validation accuracy: 84.82%

Fold:  4
--Best model validation accuracy: 86.61%

Fold:  5
--Best model validation accuracy: 76.79%

Fold:  6
--Best model validation accuracy: 77.48%

Fold:  7
--Best model validation accuracy: 81.98%

Fold:  8
--Best model validation accuracy: 82.88%

Fold:  9
--Best model validation accuracy: 84.68%


Best models average validation accuracy: 0.8263
Best models standard deviation of accuracy: 0.031417
Iteration 3 time: 643.31 seconds


~~~Grand mean of average accuracy: 0.821836
~~~Grand mean of standard deviation accuracy: 0.031839
\end{lstlisting}

\section{Weryfikacja i ocena otrzymanych rezultatów}
Przedstawione rezultaty by³y dotychczas sprawdzane jedynie na danych walidacyjnych. Prawdziwym sprawdzianem modelu jest zweryfikowanie jego skutecznoœci na wczeœniej wydzielonym, odrêbnym zbiorze testowym, który do tej pory nie bra³ udzia³u w procesie nauki. Model, który nie jest przeuczony i posiada dobr¹ zdolnoœæ generalizacji powinien osi¹gn¹æ podobn¹ skutecznoœæ dla danych testowych.

W celu weryfikacji na danych testowych, do modelu sieci wczytane zosta³y wagi zapisane na dysku, które najlepiej radzi³y sobie podczas klasyfikacji na danych walidacyjnych. Proces sprawdzania skutecznoœci na danych testowych zosta³ wywo³any osobno dla ka¿dego modelu naczonego poszczególn¹ kombinacj¹ danych. Dziêki temu z uzyskanych wyników równie¿ mo¿na policzyæ œredni¹, która bêdzie lepiej oddawa³a rzeczywisty stan, ni¿ pojedynczy pomiar. 

Na modelu wywo³ana zosta³a metoda \textit{evaluate}, której parametrami by³y dane wejœciowe oraz wyjœciowe ze zbioru testowego. Wynikiem wykonania tych poleceñ (listing \ref{lst:test_verification}) jest skutecznoœæ modelu na danych, które zosta³y u¿yte po raz pierwszy (listing \ref{lst:test_results}).

\begin{lstlisting}[caption=Sprawdzanie skutecznoœci modelu na danych testowych, language=Python, label={lst:test_verification}]
model.load_weights("tmp/best_model.h5")
model.evaluate(x_test_3d, y_test, batch_size=16, verbose=0)
\end{lstlisting}

\begin{lstlisting}[caption=Skutecznoœæ modelu na danych testowych, language=Python, label={lst:test_results}]
Iteration 1

Fold:  0
--Best model test accuracy: 77.97%

Fold:  1
--Best model test accuracy: 79.66%

Fold:  2
--Best model test accuracy: 79.66%

Fold:  3
--Best model test accuracy: 72.88%

Fold:  4
--Best model test accuracy: 79.66%

Fold:  5
--Best model test accuracy: 79.66%

Fold:  6
--Best model test accuracy: 74.58%

Fold:  7
--Best model test accuracy: 81.36%

Fold:  8
--Best model test accuracy: 76.27%

Fold:  9
--Best model test accuracy: 77.97%

Best models average test accuracy: 0.779661


Iteration 2

Fold:  0
--Best model test accuracy: 74.58%

Fold:  1
--Best model test accuracy: 79.66%

Fold:  2
--Best model test accuracy: 74.58%

Fold:  3
--Best model test accuracy: 74.58%

Fold:  4
--Best model test accuracy: 77.97%

Fold:  5
--Best model test accuracy: 72.88%

Fold:  6
--Best model test accuracy: 67.80%

Fold:  7
--Best model test accuracy: 71.19%

Fold:  8
--Best model test accuracy: 79.66%

Fold:  9
--Best model test accuracy: 74.58%

Best models average test accuracy: 0.747458


Iteration 3

Fold:  0
--Best model test accuracy: 79.66%

Fold:  1
--Best model test accuracy: 74.58%

Fold:  2
--Best model test accuracy: 76.27%

Fold:  3
--Best model test accuracy: 77.97%

Fold:  4
--Best model test accuracy: 79.66%

Fold:  5
--Best model test accuracy: 76.27%

Fold:  6
--Best model test accuracy: 72.88%

Fold:  7
--Best model test accuracy: 83.05%

Fold:  8
--Best model test accuracy: 77.97%

Fold:  9
--Best model test accuracy: 81.36%

Best models average test accuracy: 0.779661


~~~Grand mean of average test accuracy: 0.768927
\end{lstlisting}

Osi¹gniête wyniki nie s¹ identyczne jak te uzyskane na danych walidacyjnych, jednak s¹ one wystarczaj¹co podobne, aby stwierdziæ, ¿e model dobrze radzi sobie z nowymi danymi. Wynik na poziomie prawie \textbf{77\%} (76.89\%) na danych testowych jest zadowalaj¹cy i przekracza za³o¿ony próg 70\% ustalony w rozdziale \ref{expectation}.

\section{Mo¿liwoœci rozwoju}
Choæ osi¹gniêty zosta³ cel pracy, w przysz³oœci mo¿e zostaæ podjêta próba udoskonalenia wyników. Dzia³ania, które mog¹ byæ wykonane to m.in.:

\begin{itemize}
\item Próba zwalczenia zjawiska nadmiernego dopasowania dla zaproponowanego modelu,
\item Przetworzenie danych do innej postaci (\textit{ang. feature extraction}) np. wizualizacja aktywnoœci mózgu opisana w pozycji \cite{brain_visualization} lub stworzenie tzw. \textit{heat map}'y metod¹ \textit{matching pursuit} przedstawiona w ksi¹¿ce \cite{matching_pursuit},
\item Wykorzystanie gotowych modeli sieci wbudowanych w bibliotekê \textit{Keras} np. \textit{Xception}, \textit{ResNet}, \textit{VGG19} itd. opisane w dokumentacji Keras \cite{keras_applications}.
\end{itemize}
