\chapter{Wprowadzenie do g³êbokich sieci neuronowych}
\label{chapter_2}
\section{Deep learning, czyli g³êbokie sieci neuronowe}
\label{deep_neural_networks}
Deep learning to dziedzina uczenia maszynowego, która zak³ada nowe podejœcie do nauki wzorców w danych. K³adzie ona nacisk przede wszystkim na wielowarstwow¹ naukê reprezentacji danych, czyli tzw. map cech (ang. \textit{feature maps}). S³owo \textit{g³êbokie} (ang. \textit{deep}) odnosi siê g³ównie do iloœci warstw, które sk³adaj¹ siê na model sieci. Ta cecha jest zwykle nazywana g³êbokoœci¹ sieci (ang. \textit{depth}).
Nowoczesne modele g³êbokich sieci neuronowych czêsto sk³adaj¹ siê z dziesi¹tek, a czasem nawet setek warst. Ka¿da z nich uczy siê samodzielnie na podstawie przedstawionych im informacji.

Inne podejœcia, takie jak np. uczenie maszynowe oraz klasyczne sieci neuronowe skupiaj¹ siê na nauce znacznie mniejszej iloœci reprezentacji danych, dlatego tego typu metody s¹ czêsto nazywane \textit{p³ytkim uczeniem} (ang. \textit{shallow learning}).
W \textit{deep learning}'u prawie zawsze u¿ywa siê sieci neuronowych, które sk³adaj¹ siê z na³o¿onych na siebie wielu warstw neuronów.
G³êbokie sieci neuronowe dzia³aj¹ wiêc pod wieloma wzglêdami identycznie jak sieci klasyczne (patrz rysunek \ref{fig:learning_flow}).
Ró¿nicê natomiast stanowi ich wielkoœæ (g³êbokoœæ), u¿ywane metody umo¿liwiaj¹ce efektywne uczenie du¿ych struktur danych oraz nowe architektury sieci.

\begin{figure}[h!]
	\centering
	\includegraphics[width=10cm]{Rysunki/Rozdzial2/learning_flow.pdf}
	\caption{Schemat procesu uczenia g³êbokiej sieci neuronowej}
	\label{fig:learning_flow}
\end{figure}


%Ka¿da z warstw posiada zestaw tzw. wag \textit{(ang. weights)}, które okreœlaj¹ sposób w jaki interpretowane s¹ dane wejœciowe. Z technicznego punktu widzenia wagi s¹ parametrami transformacji, które s¹ wykonywane przez warstwy na danych wejœciowych. W tym kontekœcie uczenie oznacza poszukiwanie odpowiedniego zestawu wag, który pozwoli na poprawne przekszta³canie danych wejœciowych w oczekiwane rezultaty. W g³êbokich sieciach neuronowych mog¹ istnieæ miliony tego typu parametrów, wiêc znalezienie odpowiedniej wartoœci dla ka¿dego z nich mo¿e byæ trudnym zadaniem, bior¹c pod uwagê, ¿e zmiana jednego parametru mo¿e mieæ wp³yw na zachowanie innych.
%
%W celu poprawnej modyfikacji parametrów sieæ musi obserwowaæ wyniki wyjœciowe i porównywaæ je z wynikami docelowymi, które dostarczone zosta³y w procesie uczenia. Pomiarem tego jak bardzo uzyskane wyniki ró¿ni¹ siê od tych prawdziwych zajmuje siê tzw. funkcja strat \textit{(ang. loss function)}. Zwraca ona konkretn¹ wartoœæ liczbow¹ pope³nionego b³êdu, który nastêpnie mo¿e byæ minimalizowany w celu uzyskiwania coraz to lepszych wyników. Odbywa siê to poczynaj¹c od warstwy ostatniej, koñcz¹c na warstwie wejœciowej stosuj¹c algorytm wstecznej propagacji b³êdów, który implementowany jest przez tzw. optymalizator \textit{(ang. optimizer)}. \cite{Goodfellow-et-al-2016-Book}
%
%Pocz¹tkowo wagi posiadaj¹ losowe wartoœci, które dostosowywane s¹ w ramach wielu iteracji zwanych epokami ucz¹cymi \textit{(ang. epochs)}. Przewa¿nie proces powtarzany jest dziesi¹tki razy na tysi¹cach przyk³adowych danych. Wybierany jest zestaw wag sieci, który posiada najmniejsz¹ wartoœæ funkcji strat, czyli jest najlepiej dostosowany do poprawnego przetwarzania danych wejœciowych na wyjœciowe.
%Ca³y proces uczenia g³êbokiej sieci neuronowej zosta³ zaprezentowany na rysunku \ref{fig:learning_flow}.

Klasyczne sieci neuronowe swego czasu straci³y na popularnoœci, gdy¿ ciê¿ko by³o je wykorzystaæ do bardziej z³o¿onych problemów z powodu ograniczeñ ich wielkoœci.
Sieci o wiêkszych rozmiarach by³y nieefektywne, gdy¿ czasy ich nauki by³y nieakceptowalne.
Mo¿liwoœæ rozwoju g³êbokich sieci neuronowych by³a wiêc uwarunkowana powstaniem sprzêtu o odpowiedniej mocy obliczeniowej, która umo¿liwi³aby wykonywanie tak du¿ej iloœci obliczeñ w skoñczonym czasie. Obecnie u¿ywane s¹ do tego procesory kart graficznych \textit{(GPU - ang. graphics processing unit)}, które swoimi mo¿liwoœciami znacznie przewy¿szaj¹ zwyk³e procesory \textit{(CPU - central processing unit)}.

\section{Porównanie klasycznych oraz g³êbokich sieci neuronowych}
G³êbokie sieci neuronowe w swoim dzia³aniu bardzo przypominaj¹ sieci klasyczne. Wynika to z faktu, gdy¿ tak na prawdê niewiele siê od nich ró¿ni¹.
Sieæ mo¿na nazwaæ g³êbok¹, gdy posiada ona wystarczaj¹co du¿o warstw ukrytych. Iloœæ warstw klasyfikuj¹ca sieæ jako g³êbok¹ nie jest jednak ustalona. 
Zwykle przyjmuje siê, ¿e klasyczne, p³ytkie sieci neuronowe posiadaj¹ maksymalnie jedn¹ warstwê ukryt¹.
Ro¿nice w budowie klasycznych oraz g³êbokich sieci neuronowych ilustruje rysunek \ref{fig:nn_vs_dnn}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=15cm]{Rysunki/Rozdzial2/nn_vs_dnn.png}
	\caption{Porównanie struktury sieci p³ytkiej oraz g³êbokiej}
	\label{fig:nn_vs_dnn}
\end{figure}

G³êbokie sieci neuronowe nie ograniczaj¹ siê jednak do stosowania tej jednej, podstawowej struktury, która sk³ada siê z warstw neuronów po³¹czonych ka¿dy z ka¿dym (ang. \textit{fully connected}). Istniejê równie¿ inne, bardziej rozbudowane architektury sieci neuronowych, które zostan¹ opisane w rozdziale \ref{deep_arch}.

Zalet¹ klasycznych sieci neuronowych jest niew¹tpliwie ich prostota oraz szybkoœæ nauki. 
Czêsto jednak ich zastosowanie w bardziej z³o¿onych problemach bywa nieefektywne.
Przewag¹ g³êbokich sieci neuronowych jest niew¹tpliwie ich zdolnoœæ do rozwi¹zywania o wiele bardziej skomplikowanych problemów.
Przek³ada siê to oczywiœcie na d³u¿sze czasy nauki, jednak bior¹c pod uwagê aktualnie dostêpny sprzêt z du¿¹ moc¹ obliczeniow¹, s¹ one akceptowalne.

\section{Architektury g³êbokich sieci neuronowych}
\label{deep_arch}
Wyró¿nia siê 5 najczêœciej stosowanych architektur g³êbokich sieci neuronowych:
\begin{itemize}
	\item \textbf{Sieci splotowe} \textit{(CNN - ang. Convolutional Neural Networks)},
	\item \textbf{Rekurencyjne sieci neuronowe} \textit{(ang. Recurrent Neural Networks)},
	\item \textbf{ResNets} \textit{(Residual Networks)},
	\item \textbf{Autoenkodery} \textit{(ang. Autoencoders)},
	\item \textbf{GAN} \textit{(ang. Generative Adversarial Networks)}.
\end{itemize}

Zostan¹ omówione jedynie dwie pierwsze architektury, gdy¿ to one znalaz³y zastosowanie w niniejszej pracy.

\subsection{Splotowe sieci neuronowe}
Splotowe sieci neuronowe s¹ niew¹tpliwie najpopularniejsz¹ architektur¹ g³êbokich sieci neuronowych. S¹ one przewa¿nie u¿ywane do zadañ zwi¹zanych z rozpoznawaniem obrazów, jednak równie dobrze sprawdzaj¹ siê w wielu innych dziedzinach.

Przewag¹ tego typu sieci nad klasycznymi sieciami neuronowymi jest mo¿liwoœæ rozpoznawania tymczasowych oraz przestrzennych zale¿noœci pomiêdzy danymi. Jest to mo¿liwe, dziêki temu, ¿e dane które trafiaj¹ do sieci mog¹ posiadaæ strukturê wielowymiarow¹. Przyk³adowo, obrazki zapisane w formacie RGB s¹ 3-wymiarow¹ macierz¹ przechowuj¹c¹ informacjê o wartoœciach koloru ka¿dego z pikseli (rysunek \ref{fig:cnn_rgb}).

\begin{figure}[h!]
	\centering
	\includegraphics[width=6cm]{Rysunki/Rozdzial2/cnn_rgb.pdf}
	\caption{Dane liczbowe obrazka w formacie RGB}
	\label{fig:cnn_rgb}
\end{figure}

Na danych przestrzennych wykonywane s¹ operacje takie jak: konwolucja (ang. \textit{convolution}), zmniejszanie rozmiaru danych (ang. \textit{subsampling}) oraz aktywacja (ang. \textit{activation}). Znalezione cechy s¹ nastêpnie podawane na wejœcie klasycznej sieci, gdzie neurony po³¹czone s¹ ka¿dy z ka¿dym (ang. \textit{fully connected}), która ostatecznie przetwarza dane i zwraca wynik koñcowy. Dzia³anie sieci splotowej polega na tworzeniu kolejnych tzw. map cech (ang. \textit{feature map}), które s¹ abstrakcyjnymi reprezentacjami danych umo¿liwiaj¹cymi wychwycenie podobieñstw i zale¿noœci pomiêdzy danymi.
Szczegó³owy opis sposobu dzia³ania tych operacji zosta³ przedstawiony m.in. w pozycjach \cite{Chollet:2018} oraz \cite{Goodfellow-et-al-2016-Book}. 
Przyk³adowy schemat splotowej sieci neuronowej sk³adaj¹cy siê z kliku warstw konwolucyjnych po³¹czonych z warstwami zmniejszaj¹cymi rozmiar danych (tzw. \textit{pooling}'iem), która zosta³a zakoñczona dwoma warstwami sieci \textit{fully connected} (\textit{FC}) przedstawiony jest na rysunku \ref{fig:cnn}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=15cm]{Rysunki/Rozdzial2/cnn.png}
	\caption{Schemat dzia³ania sieci splotowej. ród³o: \cite{engmrk}}
	\label{fig:cnn}
\end{figure}

Na rysunku \ref{fig:cnn} mo¿na zauwa¿yæ, ¿e wymiary danych po przejœciu przez kolejne warstwy znacz¹co siê zmieniaj¹.
Ich wielkoœæ uzale¿niona jest od iloœci oraz rozmiaru zastosowanych filtrów w warstwach konwolucyjnych, które wyszukuj¹ w danych zestawów cech. 
Przy zastosowaniu du¿ej iloœci filtrów rozmiar danych ulega zwiêkszeniu. W celu ograniczenia ich wielkoœci stosowane s¹ warstwy \textit{pooling}'u, które zmniejszaj¹ ich rozmiar np. w \textit{max pooling}'u wybierana jest najwiêksza wartoœæ z danego obszaru. 
W ten sposób przygotowane cechy trafiaj¹ na warstwy \textit{fully connected}, które zwróc¹ ostateczne wyniki.

Zalet¹ tego typu sieci jest bez w¹tpienia mo¿liwoœæ przetwarzania wielowymiarowych danych, dziêki czemu s¹ one w stanie odnajdywaæ przestrzenne zale¿noœci pomiêdzy nimi. Przek³ada siê to na ich wysok¹ skutecznoœæ i wszechstronnoœæ.

\subsection{Rekurencyjne sieci neuronowe}
Rekurencyjne sieci neuronowe s¹ czêsto u¿ywane do rozwi¹zywania problemów, gdzie wykorzystywane s¹ dane, w których znacz¹c¹ rolê pe³ni ich kolejnoœæ np. tekst lub szeregi czasowe. W praktyce znajduj¹ one zastosowanie w takich problemach jak np. prognozowanie pogody czy analiza tekstu.

Idea dzia³ania rekurencyjnych sieci neuronowych polega na zapamiêtywaniu pewnego ci¹gu sekwencji i dokonywanie wyborów nie tylko na podstawie aktualnych danych, ale równie¿ tych, które by³y przetwarzane poprzednio. Jest to realizowane za pomoc¹ wewnêtrznej pêtli, która polega na przekazywaniu wyników z danej warstwy jako dodatkowych danych wejœciowych podczas kolejnej iteracji. Schemat sieciadaj¹cej pêtlê zaprezentowany zosta³ na rysunku \ref{fig:rnn}. 

\begin{figure}[H]
	\centering
	\includegraphics[width=10cm]{Rysunki/Rozdzial2/recurent_nn.png}
	\caption{Schemat rekurencyjnej sieci neuronowej}
	\label{fig:rnn}
\end{figure}

Zalet¹ rekurencyjnych sieci neuronowych jest niew¹tpliwie to, ¿e wspó³dziel¹ one wagi, dziêki temu pozwala to zredukowaæ ich iloœæ. Mog¹ byæ on równie¿ u¿ywane w po³¹czeniu z sieciami splotowymi.
Do wad nale¿y m.in. to, ¿e nie s¹ one w stanie zapamiêtywaæ zbyt d³ugich sekwencji.
